import sys
import os
import uuid
import tkinter as tk
from tkinter import ttk, filedialog, messagebox
from tkinter.scrolledtext import ScrolledText

# Try to import tkinterdnd2 for drag & drop support
try:
    from tkinterdnd2 import DND_FILES, TkinterDnD
    TKDND_AVAILABLE = True
except ImportError:
    TKDND_AVAILABLE = False
    TkinterDnD = None
import threading
import time
import concurrent.futures
import multiprocessing
import subprocess
from datetime import datetime
import numpy as np
from scipy.ndimage import gaussian_filter
from pathlib import Path
from PIL import Image, ImageDraw, ImageFont
import requests
import webbrowser
import json



# üì¶ APP VERSION INFO
APP_VERSION = "2.0.0"
# Link check update (Gist Raw)
UPDATE_URL = "https://gist.githubusercontent.com/nguyenduyducds/85850359601efe74dbcbe128cca9d7d7/raw"


# üõ†Ô∏è CONFIGURE FFMPEG FOR MOVIEPY & WHISPER (CRITICAL FOR EXE)
def get_ffmpeg_path_robust():
    """Find FFmpeg binary in Frozen (EXE) or Dev environment"""
    path = None
    
    # 1. Check PyInstaller Temp Dir (_MEI...)
    if getattr(sys, 'frozen', False):
        try:
            import imageio_ffmpeg
            # In frozen state, imageio_ffmpeg should be bundled
            # Check specifically in the _MEI folder
            base_path = sys._MEIPASS
            # Look for ffmpeg.exe recursively or in specific folders
            potential_paths = [
                os.path.join(base_path, 'ffmpeg.exe'),
                os.path.join(base_path, 'imageio_ffmpeg', 'binaries', 'ffmpeg-win64-v4.2.2.exe'), # Example common path
            ]
            
            # Use imageio's own detection if possible
            path = imageio_ffmpeg.get_ffmpeg_exe()
            print(f"   ‚ÑπÔ∏è imageio detected: {path}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è imageio lookup failed: {e}")

    # 2. Fallback: Use imageio_ffmpeg normally
    if not path or not os.path.exists(path):
        try:
            import imageio_ffmpeg
            path = imageio_ffmpeg.get_ffmpeg_exe()
        except:
            pass

    return path

try:
    ffmpeg_exe = get_ffmpeg_path_robust()
    
    if ffmpeg_exe and os.path.exists(ffmpeg_exe):
        # 1. Set environment variable for MoviePy
        os.environ["MOVIEPY_FFMPEG_BINARY"] = ffmpeg_exe
        
        # 2. Add to system PATH for Whisper/Subprocess
        ffmpeg_dir = os.path.dirname(ffmpeg_exe)
        os.environ["PATH"] += os.pathsep + ffmpeg_dir
        
        print(f"‚úÖ FFmpeg configured: {ffmpeg_exe} (Exists: {os.path.exists(ffmpeg_exe)})")
    else:
        print("‚ùå Critical: FFmpeg binary NOT found!")

except Exception as e:
    print(f"‚ö†Ô∏è FFmpeg config warning: {e}")

# Now import MoviePy (after setting env var)
# Wrap in try-except to handle PyInstaller issues
try:
    from moviepy.editor import VideoFileClip, concatenate_videoclips, AudioFileClip, TextClip, CompositeVideoClip, ColorClip, ImageClip
    from moviepy.video import fx as vfx
    from moviepy.audio import fx as afx
    print("‚úÖ MoviePy imported successfully")
except ImportError as e:
    print(f"‚ùå Critical: Failed to import MoviePy: {e}")
    print("‚ö†Ô∏è The application may not work correctly without MoviePy")
    # Define dummy classes to prevent crashes
    VideoFileClip = None
    concatenate_videoclips = None
    AudioFileClip = None
    TextClip = None
    CompositeVideoClip = None
    ColorClip = None
    ImageClip = None
    vfx = None
    afx = None

# Try to import Whisper (better accuracy)
try:
    import whisper
    WHISPER_AVAILABLE = True
    WHISPER_MODEL = None  # Will be loaded on demand
    
    # Smart Semaphore: Allow multiple Whisper instances based on RAM
    # Each Whisper model needs ~3GB RAM
    try:
        import psutil
        available_ram_gb = psutil.virtual_memory().available / (1024**3)
        max_whisper_instances = max(1, int(available_ram_gb / 3))  # 3GB per instance
        max_whisper_instances = min(max_whisper_instances, 4)  # Cap at 4 for safety
        print(f"üé§ Whisper Semaphore: {max_whisper_instances} concurrent instances (RAM: {available_ram_gb:.1f}GB)")
    except:
        max_whisper_instances = 1  # Fallback to single instance
    
    WHISPER_SEMAPHORE = threading.Semaphore(max_whisper_instances)
except ImportError:
    WHISPER_AVAILABLE = False
    WHISPER_MODEL = None
    WHISPER_SEMAPHORE = None

# Fallback to Google Speech Recognition
try:
    import speech_recognition as sr
    SPEECH_RECOGNITION_AVAILABLE = True
except ImportError:
    SPEECH_RECOGNITION_AVAILABLE = False
    sr = None

# GPU Encoding Limiter - Prevent NVENC overload
# User requested optimization for 5-10 concurrent videos
GPU_ENCODE_SEMAPHORE = threading.Semaphore(10)  # Max 10 concurrent GPU video encodes

class VideoEditorGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("Nguy·ªÖn Duy ƒê·ª©c")
        # Mobile-friendly size: 9:16 ratio (540x960)
        self.root.geometry("540x960")
        self.root.configure(bg="#0f0f23")
        
        # Variables
        # Variables
        self.input_dir = tk.StringVar(value="input/")
        self.output_dir = tk.StringVar(value="output/")
        self.start_time = tk.IntVar(value=0)
        self.duration = tk.IntVar(value=120)
        
        # Anti-copyright effects - HARD-CODED OPTIMAL VALUES
        self.blur_amount = tk.DoubleVar(value=2.5)
        self.brightness = tk.DoubleVar(value=1.0)  # 1.0 = normal (kh√¥ng thay ƒë·ªïi m√†u)
        self.zoom_factor = tk.DoubleVar(value=1.05)
        self.speed_factor = tk.DoubleVar(value=1.03)
        self.mirror_enabled = tk.BooleanVar(value=False)
        self.convert_to_portrait = tk.BooleanVar(value=True)
        self.simple_mode = tk.BooleanVar(value=False)  # NEW: Review Film Mode
        self.enable_subtitles = tk.BooleanVar(value=True)  # Required by user
        self.recognizer = None  # Will be initialized later
        
        # Additional anti-copyright effects
        self.blackwhite_enabled = tk.BooleanVar(value=False)
        self.saturation = tk.DoubleVar(value=1.0)  # 0=grayscale, 1=normal, 2=vibrant
        self.denoise_enabled = tk.BooleanVar(value=False)
        
        # üé® NEW: Advanced Color Grading
        self.contrast = tk.DoubleVar(value=1.0)  # 0.5-2.0
        self.vibrance = tk.DoubleVar(value=0)  # -100 to 100
        self.color_filter = tk.StringVar(value="None")  # None, Vintage, Cinematic, Cool, Warm
        
        # üåü NEW: Visual Effects
        self.vignette_enabled = tk.BooleanVar(value=False)
        self.vignette_strength = tk.DoubleVar(value=0.3)  # 0-1
        self.sharpen_amount = tk.DoubleVar(value=0)  # 0-5
        
        # üîÑ NEW: Transform
        self.rotation = tk.IntVar(value=0)  # 0, 90, 180, 270
        self.flip_horizontal = tk.BooleanVar(value=False)
        self.flip_vertical = tk.BooleanVar(value=False)
        self.rotate_to_portrait = tk.BooleanVar(value=False)  # Auto rotate landscape to portrait (90¬∞)
        
        # ‚è±Ô∏è NEW: Timing Effects
        self.fade_in_duration = tk.DoubleVar(value=0)  # seconds
        self.fade_out_duration = tk.DoubleVar(value=0)  # seconds
        
        # üí¨ NEW: Watermark
        self.watermark_text = tk.StringVar(value="")
        self.watermark_position = tk.StringVar(value="bottom-right")
        self.watermark_opacity = tk.DoubleVar(value=0.5)
        self.watermark_fontsize = tk.IntVar(value=24)
        self.watermark_color = tk.StringVar(value="white")
        
        
        # üîä NEW: Audio Effects
        self.audio_echo = tk.BooleanVar(value=False)
        self.bass_boost = tk.DoubleVar(value=0)  # 0-10 dB
        self.treble_boost = tk.DoubleVar(value=0)  # 0-10 dB
        self.volume_boost = tk.DoubleVar(value=1.0)  # 1.0=normal, 1.5=50% louder
        
        # Auto-edit variables
        self.auto_mode = tk.BooleanVar(value=False)
        self.watch_interval = tk.IntVar(value=5)
        self.processed_files = set()
        self.auto_running = False
        self.is_processing = False  # Initialize processing flag
        
        # üöÄ AUTO-DETECT OPTIMAL THREADS based on system specs
        optimal_threads = self.detect_optimal_threads()
        self.recommended_threads = optimal_threads  # Save for validation later
        self.num_threads = tk.IntVar(value=optimal_threads)
        
        # Add validation callback when user changes thread count
        self.num_threads.trace_add('write', lambda *args: self.validate_thread_count(self.num_threads.get()))
        
        # Intro/Outro video paths
        self.intro_video_path = tk.StringVar(value="")  # Empty = no intro
        self.outro_video_path = tk.StringVar(value="")  # Empty = no outro
        self.enable_intro = tk.BooleanVar(value=False)
        self.enable_outro = tk.BooleanVar(value=False)
        
        # Pointing hand sticker
        self.enable_pointing_hand = tk.BooleanVar(value=False)
        
        # Subtitle control
        self.enable_subtitles = tk.BooleanVar(value=True)  # Default: ON
        
        # Subtitle customization
        self.subtitle_font_size = tk.IntVar(value=14)  # Default: 14
        self.subtitle_color = tk.StringVar(value="white")  # Default: white
        self.subtitle_outline = tk.IntVar(value=3)  # Default: 3
        
        
        
        
        
        # Initialize ttk Style
        self.style = ttk.Style()
        self.style.theme_use('clam')
        
        self.setup_ui()
        self.load_speech_recognizer()
        
        # Check for updates in background
        threading.Thread(target=self.check_updates, daemon=True).start()
        
        # Create srt_files directory for subtitle storage
        os.makedirs("srt_files", exist_ok=True)
        
        # Auto-load existing videos from input directory
        self.root.after(500, self.load_existing_videos)  # Load after UI is ready
        
    
    def detect_optimal_threads(self):
        """
        üöÄ AUTO-DETECT optimal thread count based on system specs
        Returns: int (1-10)
        """
        try:
            import psutil
            
            # Get system info
            total_ram_gb = psutil.virtual_memory().total / (1024**3)
            available_ram_gb = psutil.virtual_memory().available / (1024**3)
            cpu_cores = psutil.cpu_count(logical=False) or 2  # Physical cores
            
            # Check GPU (NVIDIA NVENC)
            has_gpu = False
            try:
                import subprocess
                result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
                has_gpu = result.returncode == 0
            except:
                pass
            
            # Calculate optimal threads
            # Rule 1: RAM-based (each video + Whisper needs ~2GB)
            ram_threads = max(1, int(available_ram_gb / 2))
            
            # Rule 2: CPU-based (leave half for system)
            cpu_threads = max(1, cpu_cores // 2)
            
            # Rule 3: GPU bonus
            gpu_bonus = 3 if has_gpu else 0
            
            # Final decision: MIN of constraints, MAX 10
            optimal = min(ram_threads, cpu_threads + gpu_bonus, 10)
            optimal = max(1, optimal)  # At least 1
            
            print(f"üîç System Specs: RAM={total_ram_gb:.1f}GB (Available={available_ram_gb:.1f}GB), CPU={cpu_cores} cores, GPU={'YES' if has_gpu else 'NO'}")
            print(f"üöÄ Optimal Threads: {optimal} (RAM limit={ram_threads}, CPU limit={cpu_threads}, GPU bonus={gpu_bonus})")
            
            return optimal
        except Exception as e:
            print(f"‚ö†Ô∏è Auto-detect failed: {e}. Using default 2 threads.")
            return 2
    
    def validate_thread_count(self, new_value):
        """
        ‚ö†Ô∏è Validate thread count and warn user if exceeding safe limit
        """
        try:
            new_threads = int(float(new_value))
            safe_limit = int(self.recommended_threads * 1.5)  # 150% of recommended
            
            if new_threads > safe_limit:
                # Show warning
                result = messagebox.askyesno(
                    "‚ö†Ô∏è C·∫£nh b√°o qu√° t·∫£i!",
                    f"B·∫°n ƒëang c·ªë ch·∫°y {new_threads} lu·ªìng, v∆∞·ª£t qu√° gi·ªõi h·∫°n an to√†n ({safe_limit} lu·ªìng).\n\n"
                    f"üíª M√°y b·∫°n ƒë∆∞·ª£c khuy·∫øn ngh·ªã: {self.recommended_threads} lu·ªìng.\n\n"
                    f"‚ö†Ô∏è R·ªßi ro:\n"
                    f"  ‚Ä¢ Tr√†n RAM ‚Üí Treo m√°y\n"
                    f"  ‚Ä¢ Qu√° t·∫£i CPU/GPU ‚Üí N√≥ng m√°y\n"
                    f"  ‚Ä¢ Video b·ªã l·ªói ho·∫∑c crash\n\n"
                    f"B·∫°n c√≥ ch·∫Øc ch·∫Øn mu·ªën ti·∫øp t·ª•c?",
                    icon='warning'
                )
                
                if not result:  # User clicked "No" (H·ªßy b·ªè)
                    # Reset to recommended value
                    self.num_threads.set(self.recommended_threads)
                    self.log_console(f"‚ö†Ô∏è ƒê√£ reset v·ªÅ {self.recommended_threads} lu·ªìng (an to√†n)")
        except:
            pass
        
    
    def check_updates(self):
        """Check for updates from remote JSON"""
        try:
            response = requests.get(UPDATE_URL, timeout=5)
            if response.status_code == 200:
                data = response.json()
                latest_version = data.get("version", "0.0.0")
                download_url = data.get("download_url", "")
                message = data.get("message", "New version available!")
                
                # Check version (simple string comparison)
                # In prod, use packaging.version.parse
                if latest_version > APP_VERSION:
                    self.update_download_url = download_url
                    self.root.after(0, lambda: self.show_update_notification(latest_version, message))
        except Exception as e:
            print(f"Update check failed: {e}")

    def show_update_notification(self, version, message):
        """Show the update bar"""
        self.update_label.config(text=f"üî• Update v{version}: {message}")
        self.update_frame.pack(fill="x", before=self.root.winfo_children()[0]) # Pack at top
        
    def open_update_link(self):
        """Open download link"""
        if hasattr(self, 'update_download_url') and self.update_download_url:
            webbrowser.open(self.update_download_url)
        else:
            messagebox.showinfo("Update", "Vui l√≤ng truy c·∫≠p trang ch·ªß ƒë·ªÉ t·∫£i b·∫£n m·ªõi nh·∫•t!")
        
    def setup_ui(self):
        # Modern Title Bar with gradient effect
        title_frame = tk.Frame(self.root, bg="#1e1e3f", height=100)
        title_frame.pack(fill="x", padx=0, pady=0)
        title_frame.pack_propagate(False)
        
        # Update Notification Bar (Hidden by default)
        self.update_frame = tk.Frame(self.root, bg="#ffaa00", height=40)
        # self.update_frame.pack(fill="x") # Pack later if update found
        
        self.update_label = tk.Label(
            self.update_frame,
            text="üî• ƒê√£ c√≥ phi√™n b·∫£n m·ªõi! Nh·∫•n v√†o ƒë√¢y ƒë·ªÉ c·∫≠p nh·∫≠t.",
            font=("Segoe UI", 10, "bold"),
            bg="#ffaa00",
            fg="#000000",
            cursor="hand2"
        )
        self.update_label.pack(side="left", padx=20, pady=5)
        
        update_btn = tk.Button(
            self.update_frame,
            text="C·∫≠p nh·∫≠t ngay",
            bg="#000000",
            fg="#ffffff",
            font=("Segoe UI", 9, "bold"),
            command=self.open_update_link
        )
        update_btn.pack(side="right", padx=20, pady=5)
        
        # Click on bar to update
        self.update_label.bind("<Button-1>", lambda e: self.open_update_link())

        # Title with modern styling
        title_label = tk.Label(
            title_frame, 
            text="üé¨ Video Editor Pro",
            font=("Segoe UI", 32, "bold"),
            bg="#1e1e3f",
            fg="#00f5ff"
        )
        title_label.pack(pady=25)
        
        # Subtitle
        subtitle_label = tk.Label(
            title_frame,
            text="AI-Powered Subtitle Generator",
            font=("Segoe UI", 11),
            bg="#1e1e3f",
            fg="#8b8bff"
        )
        subtitle_label.place(relx=0.5, rely=0.7, anchor="center")
        
        # Main content container (no tabs)
        main_content = tk.Frame(self.root, bg="#0a0a1e")
        main_content.pack(fill="both", expand=True, padx=0, pady=0)
        
        # Setup Edit Video content directly
        self.setup_edit_video_tab(main_content)
    
    def setup_edit_video_tab(self, parent):
        
        # Drag & Drop Zone - Modern Design
        drop_zone_container = tk.Frame(parent, bg="#0a0a1e")
        drop_zone_container.pack(fill="x", padx=15, pady=(15, 0))
        
        self.drop_zone = tk.Frame(
            drop_zone_container,
            bg="#1a1a3e",
            relief="flat",
            bd=0,
            height=120
        )
        self.drop_zone.pack(fill="x", padx=5, pady=5)
        self.drop_zone.pack_propagate(False)
        
        # Border effect
        border_drop = tk.Frame(self.drop_zone, bg="#00f5ff", bd=0)
        border_drop.pack(fill="both", expand=True, padx=2, pady=2)
        
        inner_drop = tk.Frame(border_drop, bg="#16162e", bd=0)
        inner_drop.pack(fill="both", expand=True)
        
        # Icon and text
        drop_icon = tk.Label(
            inner_drop,
            text="üìπ",
            font=("Segoe UI", 36),
            bg="#16162e",
            fg="#00f5ff"
        )
        drop_icon.pack(pady=(15, 5))
        
        self.drop_label = tk.Label(
            inner_drop,
            text="K√©o th·∫£ video v√†o ƒë√¢y ho·∫∑c click ƒë·ªÉ ch·ªçn",
            font=("Segoe UI", 12, "bold"),
            bg="#16162e",
            fg="#8b8bff"
        )
        self.drop_label.pack(pady=(0, 5))
        
        self.drop_status = tk.Label(
            inner_drop,
            text="H·ªó tr·ª£: MP4, AVI, MOV, MKV",
            font=("Segoe UI", 9),
            bg="#16162e",
            fg="#666699"
        )
        self.drop_status.pack(pady=(0, 10))
        
        # Bind events for drag & drop (only if tkinterdnd2 is available)
        if TKDND_AVAILABLE:
            self.drop_zone.drop_target_register(DND_FILES)
            self.drop_zone.dnd_bind('<<Drop>>', self.on_drop)
            inner_drop.drop_target_register(DND_FILES)
            inner_drop.dnd_bind('<<Drop>>', self.on_drop)
        else:
            # Show message that drag & drop is not available
            self.drop_status.config(text="Click ƒë·ªÉ ch·ªçn (Drag & Drop c·∫ßn tkinterdnd2)", fg="#ffaa00")
        
        # Click to browse
        inner_drop.bind("<Button-1>", lambda e: self.browse_video_files())
        drop_icon.bind("<Button-1>", lambda e: self.browse_video_files())
        self.drop_label.bind("<Button-1>", lambda e: self.browse_video_files())
        self.drop_status.bind("<Button-1>", lambda e: self.browse_video_files())
        
        # Hover effects
        def on_enter(e):
            border_drop.config(bg="#00d4ff")
            self.drop_label.config(fg="#00f5ff")
        
        def on_leave(e):
            border_drop.config(bg="#00f5ff")
            self.drop_label.config(fg="#8b8bff")
        
        inner_drop.bind("<Enter>", on_enter)
        inner_drop.bind("<Leave>", on_leave)
        drop_icon.bind("<Enter>", on_enter)
        drop_icon.bind("<Leave>", on_leave)
        self.drop_label.bind("<Enter>", on_enter)
        self.drop_label.bind("<Leave>", on_leave)
        self.drop_status.bind("<Enter>", on_enter)
        self.drop_status.bind("<Leave>", on_leave)
        
        # Main Container with modern dark theme
        main_container = tk.Frame(parent, bg="#0a0a1e")
        main_container.pack(fill="both", expand=True, padx=15, pady=15)
        
        # Left Panel - Settings with modern card design
        left_outer = tk.Frame(main_container, bg="#16162e", width=480, relief="flat", bd=0)
        left_outer.pack(side="left", fill="both", padx=(0, 15), pady=0)
        left_outer.pack_propagate(False)
        
        # Add subtle border effect
        border_frame = tk.Frame(left_outer, bg="#2a2a4e", bd=0)
        border_frame.pack(fill="both", expand=True, padx=2, pady=2)
        
        canvas = tk.Canvas(border_frame, bg="#16162e", highlightthickness=0)
        
        # Style Custom Scrollbar (Using standard tk.Scrollbar for color support)
        scrollbar = tk.Scrollbar(border_frame, orient="vertical", command=canvas.yview, 
                                bg="#00ff88", troughcolor="#1a1a3e", width=18,
                                activebackground="#00dd77", relief="flat", bd=0)
        
        scrollable_frame = tk.Frame(canvas, bg="#16162e")
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # PACK SCROLLBAR FIRST (RIGHT), THEN CANVAS (LEFT/FILL)
        scrollbar.pack(side="right", fill="y")
        canvas.pack(side="left", fill="both", expand=True)
        
        # Mousewheel scroll - only when mouse is over canvas
        def _on_mousewheel(event):
            canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        
        def _bind_mousewheel(event):
            canvas.bind_all("<MouseWheel>", _on_mousewheel)
        
        def _unbind_mousewheel(event):
            canvas.unbind_all("<MouseWheel>")
        
        canvas.bind("<Enter>", _bind_mousewheel)
        canvas.bind("<Leave>", _unbind_mousewheel)
        
        
        self.create_settings_panel(scrollable_frame)
        
        # Right Panel - Console with modern design (Now Scrollable too!)
        right_outer = tk.Frame(main_container, bg="#16162e", relief="flat", bd=0)
        right_outer.pack(side="right", fill="both", expand=True, padx=(15, 0), pady=0)
        
        # Scrollable structure for Right Panel
        right_border = tk.Frame(right_outer, bg="#2a2a4e", bd=0)
        right_border.pack(fill="both", expand=True, padx=2, pady=2)
        
        right_canvas = tk.Canvas(right_border, bg="#16162e", highlightthickness=0)
        
        # Right Scrollbar
        right_scrollbar = tk.Scrollbar(right_border, orient="vertical", command=right_canvas.yview, 
                                bg="#00ff88", troughcolor="#1a1a3e", width=18,
                                activebackground="#00dd77", relief="flat", bd=0)
        
        right_scrollable_frame = tk.Frame(right_canvas, bg="#16162e")
        
        right_scrollable_frame.bind(
            "<Configure>",
            lambda e: right_canvas.configure(scrollregion=right_canvas.bbox("all"))
        )
        
        window_id = right_canvas.create_window((0, 0), window=right_scrollable_frame, anchor="nw")
        
        # Canvas resize handling to make inner frame fill width
        def _configure_right_canvas(event):
            right_canvas.itemconfig(window_id, width=event.width)
        
        right_canvas.bind("<Configure>", _configure_right_canvas)
        right_canvas.configure(yscrollcommand=right_scrollbar.set)
        
        # PACK SCROLLBAR FIRST
        right_scrollbar.pack(side="right", fill="y")
        right_canvas.pack(side="left", fill="both", expand=True)
        
        # Mousewheel for Right Panel
        def _on_right_mousewheel(event):
            right_canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        
        def _bind_right_mouse(event):
            right_canvas.bind_all("<MouseWheel>", _on_right_mousewheel)
        
        def _unbind_right_mouse(event):
            right_canvas.unbind_all("<MouseWheel>")
            
        right_canvas.bind("<Enter>", _bind_right_mouse)
        right_canvas.bind("<Leave>", _unbind_right_mouse)

        # Create content inside scrollable frame
        self.create_console_panel(right_scrollable_frame)
        
    def create_settings_panel(self, parent):
        settings_title = tk.Label(
            parent,
            text="‚öôÔ∏è C√ÄI ƒê·∫∂T",
            font=("Segoe UI", 16, "bold"),
            bg="#1a1a3e",
            fg="#ffffff"
        )
        settings_title.pack(pady=(15, 20))
        
        self.create_path_selector(parent, "üìÅ Th∆∞ m·ª•c Video Input:", self.input_dir, self.browse_input_dir)
        self.create_path_selector(parent, "üìÇ Th∆∞ m·ª•c Video Output:", self.output_dir, self.browse_output_dir)
        
        self.create_slider(parent, "‚è±Ô∏è B·∫Øt ƒë·∫ßu t·ª´ (gi√¢y):", self.start_time, 0, 1200)
        self.create_slider(parent, "‚è±Ô∏è Th·ªùi l∆∞·ª£ng Video (gi√¢y):", self.duration, 1, 1200)
        self.create_slider(parent, "üöÄ S·ªë Video ch·∫°y c√πng l√∫c (1-32):", self.num_threads, 1, 32)


        # GPU Acceleration
        self.use_gpu = tk.BooleanVar(value=True)  # Default ON if available
        gpu_check = tk.Checkbutton(
            parent, 
            text="üî• Render b·∫±ng GPU (NVIDIA NVENC) - Si√™u nhanh",
            variable=self.use_gpu,
            font=("Segoe UI", 10, "bold"),
            bg="#16162e",
            fg="#ffaa00",
            selectcolor="#2a2a4e",
            activebackground="#16162e",
            activeforeground="#ffaa00"
        )
        gpu_check.pack(anchor="w", padx=15, pady=5)
        
        # üé¨ INTRO/OUTRO Section
        separator_intro = tk.Frame(parent, bg="#ff00ff", height=3)
        separator_intro.pack(fill="x", padx=20, pady=20)
        
        intro_title = tk.Label(
            parent,
            text="üé¨ INTRO / OUTRO",
            font=("Segoe UI", 14, "bold"),
            bg="#16162e",
            fg="#ff00ff"
        )
        intro_title.pack(pady=(0, 15))
        
        # Intro section
        intro_frame = tk.Frame(parent, bg="#1a1a3e", relief="flat")
        intro_frame.pack(fill="x", padx=20, pady=10)
        
        intro_check = tk.Checkbutton(
            intro_frame,
            text="üé• B·∫≠t Intro (Video ƒë·∫ßu)",
            variable=self.enable_intro,
            font=("Segoe UI", 11, "bold"),
            bg="#1a1a3e",
            fg="#00ff88",
            selectcolor="#2a2a4e",
            activebackground="#1a1a3e",
            activeforeground="#00ff88"
        )
        intro_check.pack(anchor="w", padx=10, pady=5)
        
        # Intro file selector
        intro_file_frame = tk.Frame(intro_frame, bg="#1a1a3e")
        intro_file_frame.pack(fill="x", padx=10, pady=(0, 10))
        
        intro_label = tk.Label(
            intro_file_frame,
            text="üìÅ File Intro:",
            font=("Segoe UI", 10),
            bg="#1a1a3e",
            fg="#ffffff"
        )
        intro_label.pack(side="left", padx=(0, 10))
        
        intro_entry = tk.Entry(
            intro_file_frame,
            textvariable=self.intro_video_path,
            font=("Segoe UI", 9),
            bg="#0a0a1e",
            fg="#00f5ff",
            insertbackground="#00f5ff",
            relief="flat",
            bd=0
        )
        intro_entry.pack(side="left", fill="x", expand=True, ipady=6, padx=(0, 8))
        
        intro_browse_btn = tk.Button(
            intro_file_frame,
            text="üìÅ Ch·ªçn",
            font=("Segoe UI", 9, "bold"),
            bg="#00ff88",
            fg="#0a0a1e",
            activebackground="#00dd77",
            activeforeground="#ffffff",
            relief="flat",
            cursor="hand2",
            command=self.browse_intro_video,
            bd=0,
            padx=12,
            pady=4
        )
        intro_browse_btn.pack(side="right")
        
        # Outro section
        outro_frame = tk.Frame(parent, bg="#1a1a3e", relief="flat")
        outro_frame.pack(fill="x", padx=20, pady=10)
        
        outro_check = tk.Checkbutton(
            outro_frame,
            text="üé¨ B·∫≠t Outro (Video cu·ªëi)",
            variable=self.enable_outro,
            font=("Segoe UI", 11, "bold"),
            bg="#1a1a3e",
            fg="#ffaa00",
            selectcolor="#2a2a4e",
            activebackground="#1a1a3e",
            activeforeground="#ffaa00"
        )
        outro_check.pack(anchor="w", padx=10, pady=5)
        
        # Outro file selector
        outro_file_frame = tk.Frame(outro_frame, bg="#1a1a3e")
        outro_file_frame.pack(fill="x", padx=10, pady=(0, 10))
        
        outro_label = tk.Label(
            outro_file_frame,
            text="üìÅ File Outro:",
            font=("Segoe UI", 10),
            bg="#1a1a3e",
            fg="#ffffff"
        )
        outro_label.pack(side="left", padx=(0, 10))
        
        outro_entry = tk.Entry(
            outro_file_frame,
            textvariable=self.outro_video_path,
            font=("Segoe UI", 9),
            bg="#0a0a1e",
            fg="#00f5ff",
            insertbackground="#00f5ff",
            relief="flat",
            bd=0
        )
        outro_entry.pack(side="left", fill="x", expand=True, ipady=6, padx=(0, 8))
        
        outro_browse_btn = tk.Button(
            outro_file_frame,
            text="üìÅ Ch·ªçn",
            font=("Segoe UI", 9, "bold"),
            bg="#ffaa00",
            fg="#0a0a1e",
            activebackground="#dd9900",
            activeforeground="#ffffff",
            relief="flat",
            cursor="hand2",
            command=self.browse_outro_video,
            bd=0,
            padx=12,
            pady=4
        )
        outro_browse_btn.pack(side="right")
        
        
        # Anti-Copyright Section
        separator = tk.Frame(parent, bg="#00ff88", height=3)
        separator.pack(fill="x", padx=20, pady=20)
        
        anticopy_title = tk.Label(
            parent,
            text="Hi·ªáu ·ª®ng S√†i",
            font=("Segoe UI", 14, "bold"),
            bg="#1a1a3e",
            fg="#00ff88"
        )
        anticopy_title.pack(pady=(0, 10))
        
        info_frame = tk.Frame(parent, bg="#0a0a15", relief="flat")
        info_frame.pack(fill="x", padx=20, pady=10)
        
        info_text = """
‚úÖ Blur: 2.5px (l√†m m·ªù nh·∫π)
‚úÖ Brightness: +10% (s√°ng h∆°n)
‚úÖ Zoom: 1.05x (c·∫Øt vi·ªÅn)
‚úÖ Speed: 1.04x (nhanh h∆°n 4%)
‚úÖ Mirror: B·∫¨T (l·∫≠t ngang)
‚úÖ Portrait: 9:16 (cho mobile)
‚úÖ Subtitle: T·ª∞ ƒê·ªòNG (Google AI)

T·∫•t c·∫£ hi·ªáu ·ª©ng ƒë∆∞·ª£c √°p d·ª•ng t·ª± ƒë·ªông!
        """
        info_label = tk.Label(
            info_frame,
            text=info_text.strip(),
            font=("Segoe UI", 9),
            bg="#0a0a15",
            fg="#00ff88",
            justify="left",
            padx=15,
            pady=15
        )
        info_label.pack(fill="x")
        
        # Simple Mode (Review Phim)
        simple_check = tk.Checkbutton(
            parent,
            text="üé¨ Ch·∫ø ƒë·ªô Review Phim (No Subs, Black BG)",
            variable=self.simple_mode,
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#00d4ff",
            selectcolor="#2a2a4e",
            activebackground="#16162e",
            activeforeground="#00d4ff",
            command=self.toggle_simple_mode
        )
        simple_check.pack(anchor="w", padx=20, pady=(10, 5))

        # Mirror checkbox
        mirror_frame = tk.Frame(parent, bg="#1a1a3e")
        mirror_frame.pack(fill="x", padx=20, pady=10)
        mirror_check = tk.Checkbutton(
            mirror_frame,
            text="üîÑ L·∫≠t ngang (Mirror)",
            variable=self.mirror_enabled,
            font=("Segoe UI", 10, "bold"),
            bg="#1a1a3e",
            fg="#ffffff",
            selectcolor="#2a2a4e",
            activebackground="#1a1a3e",
            activeforeground="#00ff88"
        )
        mirror_check.pack(anchor="w", padx=10)
        
        # Pointing hand sticker checkbox
        pointing_check = tk.Checkbutton(
            mirror_frame,
            text="üëá Ng√≥n tay ch·ªâ xu·ªëng (Sticker)",
            variable=self.enable_pointing_hand,
            font=("Segoe UI", 10, "bold"),
            bg="#1a1a3e",
            fg="#ffaa00",
            selectcolor="#2a2a4e",
            activebackground="#1a1a3e",
            activeforeground="#ffaa00"
        )
        pointing_check.pack(anchor="w", padx=10, pady=5)
        
        
        # Brightness slider
        self.create_slider(parent, "‚òÄÔ∏è ƒê·ªô s√°ng (Brightness):", self.brightness, 0.5, 2.0, resolution=0.1)
        
        # Color Grading Sectionx
        bw_check = tk.Checkbutton(
            mirror_frame,
            text="‚ö´‚ö™ ƒêen tr·∫Øng (Black & White)",
            variable=self.blackwhite_enabled,
            font=("Segoe UI", 10, "bold"),
            bg="#1a1a3e",
            fg="#ffffff",
            selectcolor="#2a2a4e",
            activebackground="#1a1a3e",
            activeforeground="#00ff88"
        )
        bw_check.pack(anchor="w", pady=5)
        
        # Denoise checkbox
        denoise_check = tk.Checkbutton(
            mirror_frame,
            text="üîá Ch·ªëng ·ªìn (Denoise Audio)",
            variable=self.denoise_enabled,
            font=("Segoe UI", 10, "bold"),
            bg="#1a1a3e",
            fg="#ffffff",
            selectcolor="#2a2a4e",
            activebackground="#1a1a3e",
            activeforeground="#00ff88"
        )
        denoise_check.pack(anchor="w", pady=5)
        
        # Saturation slider
        self.create_slider(parent, "üé® ƒê·ªô b√£o h√≤a m√†u (0=x√°m, 2=r·ª±c r·ª°):", self.saturation, 0, 2, 0.1)
        
        # Volume boost slider  
        self.create_slider(parent, "üîä √Çm l∆∞·ª£ng (1.0=b√¨nh th∆∞·ªùng, 2.0=g·∫•p ƒë√¥i):", self.volume_boost, 0.5, 2.0, 0.1)
        
        separator2 = tk.Frame(parent, bg="#00d4ff", height=2)
        separator2.pack(fill="x", padx=20, pady=15)
        
        auto_title = tk.Label(
            parent,
            text="ü§ñ CH·∫æ ƒê·ªò T·ª∞ ƒê·ªòNG",
            font=("Segoe UI", 12, "bold"),
            bg="#1a1a3e",
            fg="#00ff88"
        )
        auto_title.pack(pady=(10, 10))
        
        auto_frame = tk.Frame(parent, bg="#1a1a3e")
        auto_frame.pack(fill="x", padx=20, pady=5)
        
        auto_label = tk.Label(auto_frame, text="üîÑ T·ª± ƒë·ªông x·ª≠ l√Ω video m·ªõi:", font=("Segoe UI", 10, "bold"), bg="#1a1a3e", fg="#ffffff")
        auto_label.pack(side="left")
        
        self.auto_status_label = tk.Label(auto_frame, text="T·∫ÆT", font=("Segoe UI", 10, "bold"), bg="#1a1a3e", fg="#ff4444")
        self.auto_status_label.pack(side="right")
        
        auto_toggle = tk.Checkbutton(auto_frame, variable=self.auto_mode, bg="#1a1a3e", activebackground="#1a1a3e", command=self.toggle_auto_mode, cursor="hand2")
        auto_toggle.pack(side="right", padx=10)
        
        self.create_slider(parent, "‚è∞ Ki·ªÉm tra m·ªói (gi√¢y):", self.watch_interval, 1, 30)
        
        # üé® NEW SECTION: Advanced Color Grading
        separator3 = tk.Frame(parent, bg="#ff00ff", height=2)
        separator3.pack(fill="x", padx=25, pady=20)
        
        color_title = tk.Label(
            parent,
            text="üé® COLOR GRADING",
            font=("Segoe UI", 14, "bold"),
            bg="#16162e",
            fg="#ff00ff"
        )
        color_title.pack(pady=(10, 15))
        
        self.create_slider(parent, "üåà Contrast (0.5=m·ªù, 2.0=s·∫Øc n√©t):", self.contrast, 0.5, 2.0, 0.1)
        self.create_slider(parent, "‚ú® Vibrance (-100 to 100):", self.vibrance, -100, 100, 5)
        self.create_dropdown(parent, "üéûÔ∏è Color Filter:", self.color_filter,
                           ["None", "Vintage", "Cinematic", "Cool", "Warm"])
        
        # üåü NEW SECTION: Visual Effects
        separator4 = tk.Frame(parent, bg="#ffaa00", height=2)
        separator4.pack(fill="x", padx=25, pady=20)
        
        vfx_title = tk.Label(
            parent,
            text="üåü VISUAL EFFECTS",
            font=("Segoe UI", 14, "bold"),
            bg="#16162e",
            fg="#ffaa00"
        )
        vfx_title.pack(pady=(10, 15))
        
        # Vignette
        vignette_frame = tk.Frame(parent, bg="#16162e")
        vignette_frame.pack(fill="x", padx=25, pady=10)
        vignette_check = tk.Checkbutton(
            vignette_frame,
            text="üåë Vignette (vi·ªÅn t·ªëi)",
            variable=self.vignette_enabled,
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#ffffff",
            selectcolor="#2a2a4e",
            activebackground="#16162e",
            activeforeground="#ffaa00"
        )
        vignette_check.pack(anchor="w")
        
        self.create_slider(parent, "üåë Vignette Strength:", self.vignette_strength, 0, 1, 0.1)
        self.create_slider(parent, "üîç Sharpen (0=none, 5=max):", self.sharpen_amount, 0, 5, 0.5)
        
        # üîÑ NEW SECTION: Transform
        separator5 = tk.Frame(parent, bg="#00ffff", height=2)
        separator5.pack(fill="x", padx=25, pady=20)
        
        transform_title = tk.Label(
            parent,
            text="üîÑ TRANSFORM",
            font=("Segoe UI", 14, "bold"),
            bg="#16162e",
            fg="#00ffff"
        )
        transform_title.pack(pady=(10, 15))
        
        # Rotation removed - caused padding issues
        # self.create_dropdown(parent, "üîÑ Rotation:", self.rotation, [0, 90, 180, 270])
        
        flip_frame = tk.Frame(parent, bg="#16162e")
        flip_frame.pack(fill="x", padx=25, pady=10)
        
        flip_h_check = tk.Checkbutton(
            flip_frame,
            text="‚ÜîÔ∏è Flip Horizontal",
            variable=self.flip_horizontal,
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#ffffff",
            selectcolor="#2a2a4e"
        )
        flip_h_check.pack(anchor="w", pady=5)
        
        flip_v_check = tk.Checkbutton(
            flip_frame,
            text="‚ÜïÔ∏è Flip Vertical",
            variable=self.flip_vertical,
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#ffffff",
            selectcolor="#2a2a4e"
        )
        flip_v_check.pack(anchor="w", pady=5)
        
        # üîÑ NEW: Rotate to Portrait
        rotate_check = tk.Checkbutton(
            flip_frame,
            text="üîÑ Xoay 90¬∞ (Ngang ‚Üí D·ªçc)",
            variable=self.rotate_to_portrait,
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#00f5ff",
            selectcolor="#2a2a4e"
        )
        rotate_check.pack(anchor="w", pady=5)
        
        # ‚è±Ô∏è NEW SECTION: Timing Effects
        separator6 = tk.Frame(parent, bg="#ff6600", height=2)
        separator6.pack(fill="x", padx=25, pady=20)
        
        timing_title = tk.Label(
            parent,
            text="‚è±Ô∏è TIMING EFFECTS",
            font=("Segoe UI", 14, "bold"),
            bg="#16162e",
            fg="#ff6600"
        )
        timing_title.pack(pady=(10, 15))
        
        self.create_slider(parent, "üì• Fade In (gi√¢y):", self.fade_in_duration, 0, 5, 0.5)
        self.create_slider(parent, "üì§ Fade Out (gi√¢y):", self.fade_out_duration, 0, 5, 0.5)
        
        # üí¨ NEW SECTION: Watermark
        separator7 = tk.Frame(parent, bg="#00ff00", height=2)
        separator7.pack(fill="x", padx=25, pady=20)
        
        watermark_title = tk.Label(
            parent,
            text="üí¨ WATERMARK",
            font=("Segoe UI", 14, "bold"),
            bg="#16162e",
            fg="#00ff00"
        )
        watermark_title.pack(pady=(10, 15))
        
        # Watermark text entry
        wm_frame = tk.Frame(parent, bg="#16162e")
        wm_frame.pack(fill="x", padx=25, pady=12)
        
        wm_label = tk.Label(
            wm_frame,
            text="üí¨ Watermark Text:",
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#ffffff"
        )
        wm_label.pack(anchor="w", pady=(0, 8))
        
        wm_entry = tk.Entry(
            wm_frame,
            textvariable=self.watermark_text,
            font=("Segoe UI", 10),
            bg="#0a0a1e",
            fg="#00f5ff",
            insertbackground="#00f5ff",
            relief="flat",
            bd=0
        )
        wm_entry.pack(fill="x", ipady=8)
        
        # Watermark Font Size
        self.create_slider(parent, "üìè C·ª° ch·ªØ Watermark:", self.watermark_fontsize, 10, 100, 1)
        
        # Watermark Color
        wm_color_frame = tk.Frame(parent, bg="#16162e")
        wm_color_frame.pack(fill="x", padx=25, pady=10)
        
        wm_color_label = tk.Label(
            wm_color_frame,
            text="üé® M√†u Watermark:",
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#ffffff"
        )
        wm_color_label.pack(anchor="w", pady=(0, 5))
        
        wm_color_dropdown = ttk.Combobox(
            wm_color_frame,
            textvariable=self.watermark_color,
            values=["white", "black", "red", "green", "blue", "yellow", "cyan", "magenta"],
            state="readonly",
            font=("Segoe UI", 10)
        )
        wm_color_dropdown.pack(fill="x", ipady=5)
        
        self.create_dropdown(parent, "üìç Watermark Position:", self.watermark_position,
                           ["top-left", "top-right", "bottom-left", "bottom-right", "center"])
        self.create_slider(parent, "üëÅÔ∏è Watermark Opacity:", self.watermark_opacity, 0, 1, 0.1)
        
        # üîä NEW SECTION: Audio Effects
        separator8 = tk.Frame(parent, bg="#ff0099", height=2)
        separator8.pack(fill="x", padx=25, pady=20)
        
        audio_title = tk.Label(
            parent,
            text="üîä AUDIO EFFECTS",
            font=("Segoe UI", 14, "bold"),
            bg="#16162e",
            fg="#ff0099"
        )
        audio_title.pack(pady=(10, 15))
        
        audio_frame = tk.Frame(parent, bg="#16162e")
        audio_frame.pack(fill="x", padx=25, pady=10)
        
        echo_check = tk.Checkbutton(
            audio_frame,
            text="üîä Audio Echo",
            variable=self.audio_echo,
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#ffffff",
            selectcolor="#2a2a4e"
        )
        echo_check.pack(anchor="w", pady=5)
        
        self.create_slider(parent, "üé∏ Bass Boost (dB):", self.bass_boost, 0, 10, 1)
        self.create_slider(parent, "üéµ Treble Boost (dB):", self.treble_boost, 0, 10, 1)
        
        # Modern Process Button with gradient effect
        process_btn = tk.Button(
            parent,
            text="üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù",
            font=("Segoe UI", 16, "bold"),
            bg="#00f5ff",
            fg="#0a0a1e",
            activebackground="#00d4ff",
            activeforeground="#ffffff",
            relief="flat",
            cursor="hand2",
            bd=0,
            command=self.start_processing
        )
        process_btn.pack(pady=25, padx=25, ipady=15, fill="x")
        
        # Hover effects
        def on_enter(e):
            process_btn.config(bg="#00d4ff", fg="#ffffff")
        def on_leave(e):
            process_btn.config(bg="#00f5ff", fg="#0a0a1e")
        
        process_btn.bind("<Enter>", on_enter)
        process_btn.bind("<Leave>", on_leave)
        
    def toggle_simple_mode(self):
        """Toggle simple mode: if ON, disable other complex effects"""
        if self.simple_mode.get():
            self.convert_to_portrait.set(False)
            self.enable_subtitles.set(False)
            # Optional: Disable other effects if needed
        else:
            self.convert_to_portrait.set(True)
            self.enable_subtitles.set(True)

    def setup_highlight_merge_tab(self, parent):
        """Setup tab for merging highlight videos - s·ª≠ d·ª•ng class ri√™ng"""
        # T·∫°o instance c·ªßa HighlightMergeTab
        self.highlight_tab = HighlightMergeTab(parent, self)




    def create_console_panel(self, parent):
        # Modern title
        console_title = tk.Label(
            parent, 
            text="üìä VIDEO STATUS MONITOR", 
            font=("Segoe UI", 18, "bold"), 
            bg="#16162e", 
            fg="#00f5ff"
        )
        console_title.pack(pady=(20, 15))
        
        # AI Engine Selector
        ai_frame = tk.Frame(parent, bg="#16162e")
        ai_frame.pack(fill="x", padx=25, pady=(0, 15))
        
        ai_label = tk.Label(
            ai_frame,
            text="ü§ñ AI Engine:",
            font=("Segoe UI", 11, "bold"),
            bg="#16162e",
            fg="#ffffff"
        )
        ai_label.pack(side="left", padx=(0, 10))
        
        # AI engine variable
        self.ai_engine = tk.StringVar(value="Whisper AI" if WHISPER_AVAILABLE else "Google Speech")
        
        ai_options = []
        if WHISPER_AVAILABLE:
            ai_options.append("Whisper AI")
        if SPEECH_RECOGNITION_AVAILABLE:
            ai_options.append("Google Speech")
        if not ai_options:
            ai_options.append("None (No Subtitles)")
        
        style = self.style
        style.configure("AI.TCombobox",
                       fieldbackground="#0a0a1e",
                       background="#2a2a4e",
                       foreground="#00f5ff",
                       arrowcolor="#00f5ff",
                       borderwidth=0)
        
        ai_dropdown = ttk.Combobox(
            ai_frame,
            textvariable=self.ai_engine,
            values=ai_options,
            state="readonly",
            font=("Segoe UI", 10),
            style="AI.TCombobox",
            width=20
        )
        ai_dropdown.pack(side="left", ipady=5)
        
        # Subtitle toggle checkbox (NEW)
        subtitle_check = tk.Checkbutton(
            ai_frame,
            text="üìù B·∫≠t Subtitle",
            variable=self.enable_subtitles,
            font=("Segoe UI", 10, "bold"),
            bg="#0a0a1e",
            fg="#00ff88",
            selectcolor="#2a2a4e",
            activebackground="#0a0a1e",
            activeforeground="#00ff88"
        )
        subtitle_check.pack(side="left", padx=20)
        
        # üìù SUBTITLE CUSTOMIZATION Section
        subtitle_custom_frame = tk.Frame(parent, bg="#1a1a3e", relief="flat")
        subtitle_custom_frame.pack(fill="x", padx=25, pady=15)
        
        subtitle_custom_title = tk.Label(
            subtitle_custom_frame,
            text="üìù T√ôY CH·ªàNH SUBTITLE",
            font=("Segoe UI", 12, "bold"),
            bg="#1a1a3e",
            fg="#00ff88"
        )
        subtitle_custom_title.pack(anchor="w", pady=(0, 10))
        
        # Font Size Slider
        fontsize_frame = tk.Frame(subtitle_custom_frame, bg="#1a1a3e")
        fontsize_frame.pack(fill="x", pady=5)
        
        fontsize_label = tk.Label(
            fontsize_frame,
            text="üìè K√≠ch th∆∞·ªõc ch·ªØ:",
            font=("Segoe UI", 10),
            bg="#1a1a3e",
            fg="#ffffff"
        )
        fontsize_label.pack(side="left", padx=(0, 10))
        
        fontsize_value = tk.Label(
            fontsize_frame,
            textvariable=self.subtitle_font_size,
            font=("Segoe UI", 10, "bold"),
            bg="#1a1a3e",
            fg="#00f5ff",
            width=3
        )
        fontsize_value.pack(side="right")
        
        fontsize_slider = tk.Scale(
            fontsize_frame,
            from_=10,
            to=30,
            orient="horizontal",
            variable=self.subtitle_font_size,
            bg="#2a2a4e",
            fg="#00f5ff",
            highlightthickness=0,
            troughcolor="#0a0a1e",
            activebackground="#00ff88",
            showvalue=False
        )
        fontsize_slider.pack(side="left", fill="x", expand=True, padx=10)
        
        # Color Dropdown
        color_frame = tk.Frame(subtitle_custom_frame, bg="#1a1a3e")
        color_frame.pack(fill="x", pady=5)
        
        color_label = tk.Label(
            color_frame,
            text="üé® M√†u ch·ªØ:",
            font=("Segoe UI", 10),
            bg="#1a1a3e",
            fg="#ffffff"
        )
        color_label.pack(side="left", padx=(0, 10))
        
        color_dropdown = ttk.Combobox(
            color_frame,
            textvariable=self.subtitle_color,
            values=["white", "yellow", "cyan", "red", "green"],
            state="readonly",
            font=("Segoe UI", 9),
            width=15
        )
        color_dropdown.pack(side="left")
        
        # Outline Slider
        outline_frame = tk.Frame(subtitle_custom_frame, bg="#1a1a3e")
        outline_frame.pack(fill="x", pady=5)
        
        outline_label = tk.Label(
            outline_frame,
            text="‚úèÔ∏è ƒê·ªô ƒë·∫≠m vi·ªÅn:",
            font=("Segoe UI", 10),
            bg="#1a1a3e",
            fg="#ffffff"
        )
        outline_label.pack(side="left", padx=(0, 10))
        
        outline_value = tk.Label(
            outline_frame,
            textvariable=self.subtitle_outline,
            font=("Segoe UI", 10, "bold"),
            bg="#1a1a3e",
            fg="#00f5ff",
            width=3
        )
        outline_value.pack(side="right")
        
        outline_slider = tk.Scale(
            outline_frame,
            from_=1,
            to=5,
            orient="horizontal",
            variable=self.subtitle_outline,
            bg="#2a2a4e",
            fg="#00f5ff",
            highlightthickness=0,
            troughcolor="#0a0a1e",
            activebackground="#00ff88",
            showvalue=False
        )
        outline_slider.pack(side="left", fill="x", expand=True, padx=10)
        
        
        
        
        # Overall Progress section
        progress_frame = tk.Frame(parent, bg="#16162e")
        progress_frame.pack(fill="x", padx=25, pady=(0, 15))
        
        self.progress_label = tk.Label(
            progress_frame, 
            text="‚ö° S·∫µn s√†ng x·ª≠ l√Ω...", 
            font=("Segoe UI", 11), 
            bg="#16162e", 
            fg="#8b8bff"
        )
        self.progress_label.pack(anchor="w", pady=(0, 8))
        
        # Modern progress bar
        style.configure("Modern.Horizontal.TProgressbar",
                       troughcolor='#0a0a1e',
                       background='#00f5ff',
                       bordercolor='#16162e',
                       lightcolor='#00f5ff',
                       darkcolor='#00d4ff')
        
        self.progress_bar = ttk.Progressbar(
            progress_frame, 
            mode="determinate",  # Changed to determinate for percentage
            length=400,
            style="Modern.Horizontal.TProgressbar"
        )
        self.progress_bar.pack(fill="x")
        
        # üéÆ VIDEO MANAGEMENT BUTTONS (moved here for visibility)
        btn_frame = tk.Frame(parent, bg="#16162e")
        btn_frame.pack(fill="x", padx=25, pady=(15, 15))
        
        # Add Video Button
        add_btn = tk.Button(
            btn_frame,
            text="‚ûï Th√™m Video",
            font=("Segoe UI", 10, "bold"),
            bg="#00ff88",
            fg="#0a0a1e",
            activebackground="#00dd77",
            activeforeground="#ffffff",
            relief="flat",
            cursor="hand2",
            command=self.add_video_manual,
            bd=0,
            padx=15,
            pady=8
        )
        add_btn.pack(side="left", padx=(0, 8), fill="x", expand=True)
        
        # Delete Selected Button
        delete_btn = tk.Button(
            btn_frame,
            text="üóëÔ∏è X√≥a ƒë√£ ch·ªçn",
            font=("Segoe UI", 10, "bold"),
            bg="#ff4444",
            fg="#ffffff",
            activebackground="#dd3333",
            activeforeground="#ffffff",
            relief="flat",
            cursor="hand2",
            command=self.delete_selected_video,
            bd=0,
            padx=15,
            pady=8
        )
        delete_btn.pack(side="left", padx=(0, 8), fill="x", expand=True)
        
        # Clear All Button
        clear_btn = tk.Button(
            btn_frame,
            text="üßπ X√≥a t·∫•t c·∫£",
            font=("Segoe UI", 10, "bold"),
            bg="#ffaa00",
            fg="#0a0a1e",
            activebackground="#dd9900",
            activeforeground="#ffffff",
            relief="flat",
            cursor="hand2",
            command=self.clear_all_videos,
            bd=0,
            padx=15,
            pady=8
        )
        clear_btn.pack(side="left", padx=(0, 8), fill="x", expand=True)
        
        # Refresh Button (NEW)
        refresh_btn = tk.Button(
            btn_frame,
            text="üîÑ L√†m m·ªõi",
            font=("Segoe UI", 10, "bold"),
            bg="#00aaff",
            fg="#ffffff",
            activebackground="#0099dd",
            activeforeground="#ffffff",
            relief="flat",
            cursor="hand2",
            command=self.refresh_video_list,
            bd=0,
            padx=15,
            pady=8
        )
        refresh_btn.pack(side="left", fill="x", expand=True)
        
        # Hover effects for buttons
        def create_hover(button, normal_bg, hover_bg):
            button.bind("<Enter>", lambda e: button.config(bg=hover_bg))
            button.bind("<Leave>", lambda e: button.config(bg=normal_bg))
        
        create_hover(add_btn, "#00ff88", "#00dd77")
        create_hover(delete_btn, "#ff4444", "#dd3333")
        create_hover(clear_btn, "#ffaa00", "#dd9900")
        create_hover(refresh_btn, "#00aaff", "#0099dd")
        
        
        # Video Status Table (Treeview)
        table_frame = tk.Frame(parent, bg="#0a0a1e", relief="flat", bd=0)
        table_frame.pack(fill="both", expand=True, padx=25, pady=(0, 25))
        
        # Configure Treeview style
        style.configure("Video.Treeview",
                       background="#050510",
                       foreground="#00f5ff",
                       fieldbackground="#050510",
                       borderwidth=0,
                       font=("Segoe UI", 10))
        style.configure("Video.Treeview.Heading",
                       background="#1a1a3e",
                       foreground="#ffffff",
                       borderwidth=0,
                       font=("Segoe UI", 11, "bold"))
        style.map("Video.Treeview",
                 background=[("selected", "#2a2a4e")],
                 foreground=[("selected", "#00f5ff")])
        
        
        # Create Treeview with scrollbar
        tree_scroll = tk.Scrollbar(table_frame, bg="#2a2a4e", troughcolor="#16162e", width=12)
        tree_scroll.pack(side="right", fill="y")
        
        # Define columns - EXPANDED with more info
        columns = ("video", "status", "progress", "time", "size", "resolution", "speed", "ai")
        self.video_tree = ttk.Treeview(
            table_frame,
            columns=columns,
            show="headings",
            style="Video.Treeview",
            yscrollcommand=tree_scroll.set,
            height=15
        )
        
        tree_scroll.config(command=self.video_tree.yview)
        
        # Define headings
        self.video_tree.heading("video", text="üìπ Video Name")
        self.video_tree.heading("status", text="üìä Status")
        self.video_tree.heading("progress", text="‚è≥ Progress")
        self.video_tree.heading("time", text="‚è±Ô∏è Time")
        self.video_tree.heading("size", text="üìè Size")
        self.video_tree.heading("resolution", text="üéûÔ∏è Resolution")
        self.video_tree.heading("speed", text="‚ö° Speed")
        self.video_tree.heading("ai", text="ü§ñ AI")
        
        # Define column widths
        self.video_tree.column("video", width=200, anchor="w")
        self.video_tree.column("status", width=120, anchor="center")
        self.video_tree.column("progress", width=80, anchor="center")
        self.video_tree.column("time", width=70, anchor="center")
        self.video_tree.column("size", width=80, anchor="center")
        self.video_tree.column("resolution", width=100, anchor="center")
        self.video_tree.column("speed", width=80, anchor="center")
        self.video_tree.column("ai", width=90, anchor="center")
        
        self.video_tree.pack(fill="both", expand=True, padx=3, pady=3)
        
        # Store video items for updates
        self.video_items = {}  # {filename: {id, start_time, status, size, resolution, etc.}}
        
        # Add welcome message
        welcome_id = self.video_tree.insert("", "end", values=(
            "üé¨ Video Editor Pro - Ready",
            "‚ö° Waiting",
            "0%",
            "0s",
            "-",
            "-",
            "-",
            "-"
        ))
        



    def create_path_selector(self, parent, label_text, variable, command):
        frame = tk.Frame(parent, bg="#16162e")
        frame.pack(fill="x", padx=25, pady=12)
        
        label = tk.Label(
            frame, 
            text=label_text, 
            font=("Segoe UI", 11, "bold"), 
            bg="#16162e", 
            fg="#ffffff"
        )
        label.pack(anchor="w", pady=(0, 8))
        
        entry_frame = tk.Frame(frame, bg="#16162e")
        entry_frame.pack(fill="x")
        
        entry = tk.Entry(
            entry_frame, 
            textvariable=variable, 
            font=("Segoe UI", 10), 
            bg="#0a0a1e", 
            fg="#00f5ff", 
            insertbackground="#00f5ff", 
            relief="flat",
            bd=0
        )
        entry.pack(side="left", fill="x", expand=True, ipady=8, padx=(0, 8))
        
        browse_btn = tk.Button(
            entry_frame, 
            text="üìÅ", 
            font=("Segoe UI", 12), 
            bg="#2a2a4e", 
            fg="#00f5ff", 
            activebackground="#3a3a5e", 
            activeforeground="#ffffff",
            relief="flat", 
            cursor="hand2", 
            command=command, 
            width=3,
            bd=0
        )
        browse_btn.pack(side="right")
        
        # Hover effect
        browse_btn.bind("<Enter>", lambda e: browse_btn.config(bg="#3a3a5e"))
        browse_btn.bind("<Leave>", lambda e: browse_btn.config(bg="#2a2a4e"))
        
    def create_slider(self, parent, label_text, variable, from_, to, resolution=1):
        frame = tk.Frame(parent, bg="#16162e")
        frame.pack(fill="x", padx=25, pady=12)
        
        # Label row with modern styling
        label_frame = tk.Frame(frame, bg="#16162e")
        label_frame.pack(fill="x", pady=(0, 8))
        
        label = tk.Label(
            label_frame, 
            text=label_text, 
            font=("Segoe UI", 11, "bold"), 
            bg="#16162e", 
            fg="#ffffff"
        )
        label.pack(side="left")
        
        # üÜï Input Entry for direct number input (NEW!)
        entry_frame = tk.Frame(label_frame, bg="#16162e")
        entry_frame.pack(side="right")
        
        # Entry widget for typing numbers
        entry = tk.Entry(
            entry_frame,
            font=("Segoe UI", 11, "bold"),
            bg="#0a0a1e",
            fg="#00f5ff",
            insertbackground="#00f5ff",
            relief="flat",
            bd=0,
            width=6,
            justify="center"
        )
        entry.pack(side="left", padx=(0, 8), ipady=2)
        entry.insert(0, str(int(variable.get())))
        
        # Value display with modern badge style (kept for visual consistency)
        value_label = tk.Label(
            entry_frame, 
            text=str(int(variable.get())), 
            font=("Segoe UI", 11, "bold"), 
            bg="#00f5ff", 
            fg="#0a0a1e",
            padx=12,
            pady=2
        )
        value_label.pack(side="left")
        
        # Update functions for 2-way sync
        def update_from_slider(v):
            """Update when slider moves"""
            val = int(float(v))
            value_label.config(text=str(val))
            entry.delete(0, tk.END)
            entry.insert(0, str(val))
        
        def update_from_entry(event=None):
            """Update when user types in entry"""
            try:
                val = int(entry.get())
                # Clamp value to valid range
                val = max(from_, min(to, val))
                variable.set(val)
                value_label.config(text=str(val))
                entry.delete(0, tk.END)
                entry.insert(0, str(val))
            except ValueError:
                # Invalid input, reset to current value
                entry.delete(0, tk.END)
                entry.insert(0, str(int(variable.get())))
        
        # Bind Enter key to update
        entry.bind("<Return>", update_from_entry)
        entry.bind("<FocusOut>", update_from_entry)
        
        # Modern slider with ttk
        self.style.configure("Modern.Horizontal.TScale",
                       background="#16162e",
                       troughcolor="#0a0a1e",
                       borderwidth=0,
                       lightcolor="#00f5ff",
                       darkcolor="#00d4ff")
        
        slider = ttk.Scale(
            frame, 
            from_=from_, 
            to=to, 
            orient="horizontal", 
            variable=variable,
            style="Modern.Horizontal.TScale",
            command=update_from_slider
        )
        slider.pack(fill="x", pady=(0, 5))
        
        
    def create_dropdown(self, parent, label_text, variable, values):
        frame = tk.Frame(parent, bg="#16162e")
        frame.pack(fill="x", padx=25, pady=12)
        
        label = tk.Label(
            frame, 
            text=label_text, 
            font=("Segoe UI", 11, "bold"), 
            bg="#16162e", 
            fg="#ffffff"
        )
        label.pack(anchor="w", pady=(0, 8))
        
        # Modern dropdown style
        self.style.configure("Modern.TCombobox",
                       fieldbackground="#0a0a1e",
                       background="#2a2a4e",
                       foreground="#00f5ff",
                       arrowcolor="#00f5ff",
                       borderwidth=0)
        
        dropdown = ttk.Combobox(
            frame, 
            textvariable=variable, 
            values=values, 
            state="readonly", 
            font=("Segoe UI", 10),
            style="Modern.TCombobox"
        )
        dropdown.pack(fill="x", ipady=5)
        
    def browse_input_dir(self):
        directory = filedialog.askdirectory(title="Ch·ªçn th∆∞ m·ª•c Input")
        if directory:
            self.input_dir.set(directory)
            
    def browse_output_dir(self):
        directory = filedialog.askdirectory(title="Ch·ªçn th∆∞ m·ª•c Output")
        if directory:
            self.output_dir.set(directory)
    
    def browse_intro_video(self):
        """Browse and select intro video file"""
        filename = filedialog.askopenfilename(
            title="Ch·ªçn video Intro (ƒë·∫ßu)",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv"),
                ("All files", "*.*")
            ]
        )
        if filename:
            self.intro_video_path.set(filename)
            self.log_console(f"‚úÖ ƒê√£ ch·ªçn Intro: {os.path.basename(filename)}")
    
    def browse_outro_video(self):
        """Browse and select outro video file"""
        filename = filedialog.askopenfilename(
            title="Ch·ªçn video Outro (cu·ªëi)",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv"),
                ("All files", "*.*")
            ]
        )
        if filename:
            self.outro_video_path.set(filename)
            self.log_console(f"‚úÖ ƒê√£ ch·ªçn Outro: {os.path.basename(filename)}")
    
    
    def browse_video_files(self):
        """Browse and select video files to copy to input directory"""
        filenames = filedialog.askopenfilenames(
            title="Ch·ªçn video ƒë·ªÉ x·ª≠ l√Ω",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv *.flv *.wmv"),
                ("All files", "*.*")
            ]
        )
        if filenames:
            self.copy_videos_to_input(filenames)
    
    def on_drop(self, event):
        """Handle drag and drop event"""
        # Get dropped files
        files = self.root.tk.splitlist(event.data)
        
        # Filter video files
        video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.MP4', '.AVI', '.MOV', '.MKV', '.FLV', '.WMV')
        video_files = [f for f in files if f.lower().endswith(video_extensions)]
        
        if video_files:
            self.copy_videos_to_input(video_files)
        else:
            self.drop_status.config(text="‚ùå Kh√¥ng c√≥ video h·ª£p l·ªá!", fg="#ff4444")
            self.root.after(3000, lambda: self.drop_status.config(text="H·ªó tr·ª£: MP4, AVI, MOV, MKV", fg="#666699"))
    
    def copy_videos_to_input(self, file_paths):
        """Copy selected/dropped videos to input directory"""
        import shutil
        
        # Create input directory if it doesn't exist
        input_dir = self.input_dir.get()
        if not os.path.exists(input_dir):
            os.makedirs(input_dir)
        
        copied_count = 0
        for file_path in file_paths:
            try:
                # Get filename
                filename = os.path.basename(file_path)
                dest_path = os.path.join(input_dir, filename)
                
                # Copy file
                if not os.path.exists(dest_path):
                    shutil.copy2(file_path, dest_path)
                    copied_count += 1
                    self.log_console(f"‚úÖ ƒê√£ copy: {filename}")
                else:
                    self.log_console(f"‚ö†Ô∏è File ƒë√£ t·ªìn t·∫°i: {filename}")
            except Exception as e:
                self.log_console(f"‚ùå L·ªói copy {filename}: {str(e)}")
        
        # Update status
        if copied_count > 0:
            self.drop_status.config(text=f"‚úÖ ƒê√£ th√™m {copied_count} video!", fg="#00ff88")
            self.drop_label.config(text=f"ƒê√£ th√™m {copied_count} video v√†o th∆∞ m·ª•c input")
            self.root.after(3000, lambda: self.drop_label.config(text="K√©o th·∫£ video v√†o ƒë√¢y ho·∫∑c click ƒë·ªÉ ch·ªçn"))
            self.root.after(3000, lambda: self.drop_status.config(text="H·ªó tr·ª£: MP4, AVI, MOV, MKV", fg="#666699"))
        else:
            self.drop_status.config(text="‚ö†Ô∏è Kh√¥ng c√≥ video m·ªõi ƒë∆∞·ª£c th√™m", fg="#ffaa00")
            self.root.after(3000, lambda: self.drop_status.config(text="H·ªó tr·ª£: MP4, AVI, MOV, MKV", fg="#666699"))
            
            
            
    def log_console(self, message):
        """Legacy function - now just prints to stdout for debugging"""
        print(message)
    
    def add_video_to_tree(self, filename):
        """Add a video to the status tree with full info"""
        def _add():
            if filename not in self.video_items:
                # Get video info
                input_path = os.path.join(self.input_dir.get(), filename)
                size_mb = "-"
                resolution = "-"
                
                try:
                    # Get file size
                    if os.path.exists(input_path):
                        size_bytes = os.path.getsize(input_path)
                        size_mb = f"{size_bytes / (1024*1024):.1f} MB"
                        
                        # Get resolution using moviepy
                        try:
                            with VideoFileClip(input_path) as clip:
                                resolution = f"{clip.w}x{clip.h}"
                        except:
                            resolution = "Unknown"
                except:
                    pass
                
                # Get AI engine
                ai_engine = self.ai_engine.get() if hasattr(self, 'ai_engine') else "-"
                if "Whisper" in ai_engine:
                    ai_engine = "Whisper"
                elif "Google" in ai_engine:
                    ai_engine = "Google"
                else:
                    ai_engine = "None"
                
                item_id = self.video_tree.insert("", "end", values=(
                    filename,
                    "‚è≥ Queued",
                    "0%",
                    "0s",
                    size_mb,
                    resolution,
                    "-",
                    ai_engine
                ))
                self.video_items[filename] = {
                    'id': item_id,
                    'start_time': None,
                    'status': 'queued',
                    'size': size_mb,
                    'resolution': resolution,
                    'ai': ai_engine
                }
        self.root.after(0, _add)
    
    def add_video_manual(self):
        """Manually add videos to the input directory and tree"""
        filenames = filedialog.askopenfilenames(
            title="Ch·ªçn video ƒë·ªÉ th√™m v√†o danh s√°ch",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv *.flv *.wmv"),
                ("All files", "*.*")
            ]
        )
        if filenames:
            import shutil
            input_dir = self.input_dir.get()
            if not os.path.exists(input_dir):
                os.makedirs(input_dir)
            
            added_count = 0
            for file_path in filenames:
                try:
                    filename = os.path.basename(file_path)
                    dest_path = os.path.join(input_dir, filename)
                    
                    # Copy file if not exists
                    if not os.path.exists(dest_path):
                        shutil.copy2(file_path, dest_path)
                        self.log_console(f"‚úÖ ƒê√£ th√™m: {filename}")
                        added_count += 1
                        
                        # Add to tree
                        self.add_video_to_tree(filename)
                    else:
                        self.log_console(f"‚ö†Ô∏è File ƒë√£ t·ªìn t·∫°i: {filename}")
                        # Still add to tree if not already there
                        if filename not in self.video_items:
                            self.add_video_to_tree(filename)
                            added_count += 1
                except Exception as e:
                    self.log_console(f"‚ùå L·ªói th√™m {filename}: {str(e)}")
            
            if added_count > 0:
                messagebox.showinfo("Th√†nh c√¥ng", f"ƒê√£ th√™m {added_count} video v√†o danh s√°ch!")
            else:
                messagebox.showwarning("C·∫£nh b√°o", "Kh√¥ng c√≥ video m·ªõi ƒë∆∞·ª£c th√™m!")
    
    def delete_selected_video(self):
        """Delete selected video from tree and optionally from disk - REAL-TIME"""
        selected = self.video_tree.selection()
        if not selected:
            messagebox.showwarning("C·∫£nh b√°o", "Vui l√≤ng ch·ªçn video c·∫ßn x√≥a!")
            return
        
        # Get selected video info
        item = selected[0]
        values = self.video_tree.item(item, 'values')
        filename = values[0]
        
        # Skip welcome message
        if "Video Editor Pro" in filename:
            messagebox.showwarning("C·∫£nh b√°o", "Kh√¥ng th·ªÉ x√≥a d√≤ng n√†y!")
            return
        
        # Confirm deletion
        response = messagebox.askyesnocancel(
            "X√°c nh·∫≠n x√≥a",
            f"B·∫°n mu·ªën x√≥a '{filename}'?\n\n"
            "‚Ä¢ YES: X√≥a kh·ªèi danh s√°ch V√Ä x√≥a file\n"
            "‚Ä¢ NO: Ch·ªâ x√≥a kh·ªèi danh s√°ch\n"
            "‚Ä¢ CANCEL: H·ªßy"
        )
        
        if response is None:  # Cancel
            return
        
        # ‚ú® REAL-TIME: Remove from tree IMMEDIATELY (before file deletion)
        self.video_tree.delete(item)
        
        # Remove from video_items dict
        if filename in self.video_items:
            del self.video_items[filename]
        
        # Delete file if user chose YES (in background, UI already updated)
        if response:  # YES
            try:
                input_path = os.path.join(self.input_dir.get(), filename)
                if os.path.exists(input_path):
                    os.remove(input_path)
                    self.log_console(f"üóëÔ∏è ƒê√£ x√≥a file: {filename}")
                else:
                    self.log_console(f"üìù ƒê√£ x√≥a kh·ªèi danh s√°ch: {filename}")
            except Exception as e:
                self.log_console(f"‚ùå L·ªói x√≥a file {filename}: {str(e)}")
                # Don't show error dialog, just log it
        else:  # NO
            self.log_console(f"üìù ƒê√£ x√≥a kh·ªèi danh s√°ch: {filename}")
    
    
    def clear_all_videos(self):
        """Clear all videos from tree and optionally from disk"""
        # Count videos (exclude welcome message)
        all_items = self.video_tree.get_children()
        video_count = 0
        for item in all_items:
            values = self.video_tree.item(item, 'values')
            if "Video Editor Pro" not in values[0]:
                video_count += 1
        
        if video_count == 0:
            messagebox.showinfo("Th√¥ng b√°o", "Danh s√°ch ƒëang tr·ªëng!")
            return
        
        # Confirm deletion
        response = messagebox.askyesnocancel(
            "X√°c nh·∫≠n x√≥a t·∫•t c·∫£",
            f"B·∫°n mu·ªën x√≥a {video_count} video?\n\n"
            "‚Ä¢ YES: X√≥a kh·ªèi danh s√°ch V√Ä x√≥a file\n"
            "‚Ä¢ NO: Ch·ªâ x√≥a kh·ªèi danh s√°ch\n"
            "‚Ä¢ CANCEL: H·ªßy"
        )
        
        if response is None:  # Cancel
            return
        
        deleted_count = 0
        failed_count = 0
        
        # Get all items to delete (except welcome message)
        items_to_delete = []
        for item in all_items:
            values = self.video_tree.item(item, 'values')
            filename = values[0]
            if "Video Editor Pro" not in filename:
                items_to_delete.append((item, filename))
        
        # Delete items
        for item, filename in items_to_delete:
            # Remove from tree
            self.video_tree.delete(item)
            
            # Remove from dict
            if filename in self.video_items:
                del self.video_items[filename]
            
            # Delete file if user chose YES
            if response:  # YES
                try:
                    input_path = os.path.join(self.input_dir.get(), filename)
                    if os.path.exists(input_path):
                        os.remove(input_path)
                        self.log_console(f"üóëÔ∏è ƒê√£ x√≥a: {filename}")
                        deleted_count += 1
                    else:
                        deleted_count += 1
                except Exception as e:
                    self.log_console(f"‚ùå L·ªói x√≥a {filename}: {str(e)}")
                    failed_count += 1
            else:  # NO
                deleted_count += 1
        
        # Show result
        if response:  # YES
            if failed_count > 0:
                messagebox.showwarning(
                    "Ho√†n t·∫•t",
                    f"ƒê√£ x√≥a {deleted_count} video!\n{failed_count} video kh√¥ng th·ªÉ x√≥a file."
                )
            else:
                messagebox.showinfo("Th√†nh c√¥ng", f"ƒê√£ x√≥a {deleted_count} video kh·ªèi danh s√°ch v√† ·ªï ƒëƒ©a!")
        else:  # NO
            messagebox.showinfo("Th√†nh c√¥ng", f"ƒê√£ x√≥a {deleted_count} video kh·ªèi danh s√°ch!")
    
    def load_existing_videos(self):
        """Auto-load existing videos from input directory on startup"""
        try:
            input_dir = self.input_dir.get()
            if not os.path.exists(input_dir):
                os.makedirs(input_dir)
                self.log_console("üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c input")
                return
            
            # Get all video files
            video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')
            video_files = []
            
            for file in os.listdir(input_dir):
                if file.lower().endswith(video_extensions):
                    video_files.append(file)
            
            if video_files:
                self.log_console(f"üìÇ T√¨m th·∫•y {len(video_files)} video trong th∆∞ m·ª•c input")
                
                # Add each video to tree
                for filename in sorted(video_files):
                    self.add_video_to_tree(filename)
                
                self.log_console(f"‚úÖ ƒê√£ load {len(video_files)} video v√†o danh s√°ch")
            else:
                self.log_console("üìÇ Th∆∞ m·ª•c input ƒëang tr·ªëng")
                
        except Exception as e:
            self.log_console(f"‚ö†Ô∏è L·ªói load video: {str(e)}")
    
    def refresh_video_list(self):
        """Refresh video list - scan input directory and update tree"""
        try:
            input_dir = self.input_dir.get()
            if not os.path.exists(input_dir):
                messagebox.showwarning("C·∫£nh b√°o", "Th∆∞ m·ª•c input kh√¥ng t·ªìn t·∫°i!")
                return
            
            # Get all video files from directory
            video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')
            files_in_dir = set()
            
            for file in os.listdir(input_dir):
                if file.lower().endswith(video_extensions):
                    files_in_dir.add(file)
            
            # Get current videos in tree (exclude welcome message)
            current_videos = set()
            all_items = self.video_tree.get_children()
            for item in all_items:
                values = self.video_tree.item(item, 'values')
                filename = values[0]
                if "Video Editor Pro" not in filename:
                    current_videos.add(filename)
            
            # Find new videos (in directory but not in tree)
            new_videos = files_in_dir - current_videos
            
            # Find deleted videos (in tree but not in directory)
            deleted_videos = current_videos - files_in_dir
            
            # Add new videos
            for filename in sorted(new_videos):
                self.add_video_to_tree(filename)
                self.log_console(f"‚ûï Th√™m m·ªõi: {filename}")
            
            # Remove deleted videos
            for filename in deleted_videos:
                if filename in self.video_items:
                    item_id = self.video_items[filename]['id']
                    self.video_tree.delete(item_id)
                    del self.video_items[filename]
                    self.log_console(f"üóëÔ∏è ƒê√£ x√≥a (kh√¥ng t·ªìn t·∫°i): {filename}")
            
            # Show summary
            if new_videos or deleted_videos:
                msg = f"üîÑ L√†m m·ªõi ho√†n t·∫•t!\n\n"
                if new_videos:
                    msg += f"‚ûï Th√™m m·ªõi: {len(new_videos)} video\n"
                if deleted_videos:
                    msg += f"üóëÔ∏è ƒê√£ x√≥a: {len(deleted_videos)} video\n"
                self.log_console(msg.strip())
                # Show brief notification (auto-close after 1.5s)
                self.show_brief_notification(msg.strip())
            else:
                self.log_console("üîÑ Danh s√°ch ƒë√£ c·∫≠p nh·∫≠t (kh√¥ng c√≥ thay ƒë·ªïi)")
                self.show_brief_notification("‚úÖ Danh s√°ch ƒë√£ c·∫≠p nh·∫≠t")
                
        except Exception as e:
            self.log_console(f"‚ùå L·ªói l√†m m·ªõi: {str(e)}")
            messagebox.showerror("L·ªói", f"Kh√¥ng th·ªÉ l√†m m·ªõi danh s√°ch: {str(e)}")
    
    def show_brief_notification(self, message):
        """Show a brief notification that auto-closes"""
        # Create a simple top-level window
        notif = tk.Toplevel(self.root)
        notif.title("Th√¥ng b√°o")
        notif.geometry("300x100")
        notif.configure(bg="#1e1e3f")
        notif.resizable(False, False)
        
        # Center on screen
        notif.update_idletasks()
        x = (notif.winfo_screenwidth() // 2) - (300 // 2)
        y = (notif.winfo_screenheight() // 2) - (100 // 2)
        notif.geometry(f"300x100+{x}+{y}")
        
        # Message label
        label = tk.Label(
            notif,
            text=message,
            font=("Segoe UI", 11),
            bg="#1e1e3f",
            fg="#00f5ff",
            wraplength=280,
            justify="center"
        )
        label.pack(expand=True, pady=20)
        
        # Auto-close after 1.5 seconds
        notif.after(1500, notif.destroy)
    
    
    
    
    def update_video_status(self, filename, status, progress="", elapsed_time="", speed=""):
        """Update video status in the tree with all columns"""
        def _update():
            if filename in self.video_items:
                item_id = self.video_items[filename]['id']
                
                # Get current values
                current = self.video_tree.item(item_id)['values']
                
                # Update values (preserve existing if not provided)
                new_progress = progress if progress else current[2]
                new_time = elapsed_time if elapsed_time else current[3]
                new_speed = speed if speed else current[6]
                
                self.video_tree.item(item_id, values=(
                    filename,
                    status,
                    new_progress,
                    new_time,
                    current[4],  # size (unchanged)
                    current[5],  # resolution (unchanged)
                    new_speed,
                    current[7]   # ai (unchanged)
                ))
                
                # Update status in dict
                self.video_items[filename]['status'] = status
        self.root.after(0, _update)
    
    def update_video_progress(self, filename, progress_percent, elapsed_time, speed=""):
        """Update video progress percentage, time, and speed"""
        def _update():
            if filename in self.video_items:
                item_id = self.video_items[filename]['id']
                current = self.video_tree.item(item_id)['values']
                
                speed_text = speed if speed else current[6]
                
                self.video_tree.item(item_id, values=(
                    filename,
                    current[1],  # Keep current status
                    f"{progress_percent}%",
                    f"{elapsed_time}s",
                    current[4],  # size
                    current[5],  # resolution
                    speed_text,
                    current[7]   # ai
                ))
        self.root.after(0, _update)
    
    
    def update_overall_progress(self, completed, total):
        """Update overall progress bar"""
        def _update():
            if total > 0:
                percent = (completed / total) * 100
                self.progress_bar['value'] = percent
                self.progress_label.config(text=f"‚ö° ƒê√£ x·ª≠ l√Ω: {completed}/{total} video ({percent:.0f}%)")
            else:
                self.progress_bar['value'] = 0
                self.progress_label.config(text="‚ö° S·∫µn s√†ng x·ª≠ l√Ω...")
        self.root.after(0, _update)
    
    
    
    def load_speech_recognizer(self):
        """Initialize Speech Recognizer"""
        if not SPEECH_RECOGNITION_AVAILABLE:
            self.log_console("‚ö†Ô∏è SpeechRecognition ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ph·ª• ƒë·ªÅ t·ª± ƒë·ªông s·∫Ω b·ªã t·∫Øt.\n")
            self.recognizer = None
            return
        
        try:
            self.recognizer = sr.Recognizer()
            self.log_console("‚úÖ Speech Recognizer s·∫µn s√†ng!\n")
        except Exception as e:
            self.log_console(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o recognizer: {str(e)}\n")
            self.recognizer = None
        
    def convert_to_portrait_mode(self, video):
        """Chuy·ªÉn video sang ƒë·ªãnh d·∫°ng d·ªçc 9:16 (1080x1920) - Optimized for speed"""
        target_w, target_h = 1080, 1920
        orig_w, orig_h = video.size

        # Scale: width 100%, height 230%
        scale_w = target_w / orig_w
        new_w = target_w
        new_h = int(orig_h * scale_w * 2.3)  # Height x2.3
        
        video_main = video.resized((new_w, new_h))

        # Create blurred background - DYNAMIC (Moving with video)
        # Resize extremely small for speed (10% size) -> Super fast processing
        bg_w = target_w // 10
        bg_h = target_h // 10
        bg_video = video.resized((bg_w, bg_h))
        
        def blur_background(get_frame, t):
            frame = get_frame(t)
            # Blur spatial dims (height, width) but NOT color channel (last dim)
            # sigma=(3, 3, 0) is fast and effective on small images
            blurred = gaussian_filter(frame, sigma=(3, 3, 0))
            # Darken to make main video pop
            return (blurred * 0.4).astype('uint8')
        
        bg_video = bg_video.transform(blur_background)
        bg_video = bg_video.resized((target_w, target_h))

        # Crop video to fit height (center crop)
        if new_h >= target_h:
            y1 = (new_h - target_h) // 2
            video_main = video_main.cropped(x1=0, y1=y1, x2=target_w, y2=y1+target_h)
            return video_main
        else:
            # If still shorter, composite on background
            y_pos = (target_h - new_h) // 2
            video_main = video_main.with_position(('center', y_pos))
            return CompositeVideoClip([bg_video, video_main], size=(target_w, target_h)).with_duration(video.duration)
    

        
    def _create_srt_file(self, text, start_offset, end_offset, output_path):
        """Create SRT subtitle file from text (for FFmpeg burning)"""
        try:
            # Split text into chunks (30 chars max)
            words = text.split()
            chunks = []
            current_chunk = []
            
            for word in words:
                candidate = current_chunk + [word]
                if len(' '.join(candidate)) <= 30:
                    current_chunk.append(word)
                else:
                    if current_chunk:
                        chunks.append(' '.join(current_chunk))
                    current_chunk = [word]
            
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            
            # Create SRT content
            duration = end_offset - start_offset
            duration_per_chunk = duration / len(chunks) if chunks else 0
            
            srt_content = []
            for i, chunk_text in enumerate(chunks):
                start_time = start_offset + (i * duration_per_chunk)
                end_time = start_offset + ((i + 1) * duration_per_chunk)
                
                # SRT format
                def format_time(seconds):
                    hours = int(seconds // 3600)
                    minutes = int((seconds % 3600) // 60)
                    secs = int(seconds % 60)
                    millis = int((seconds % 1) * 1000)
                    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
                
                srt_content.append(f"{i + 1}")
                srt_content.append(f"{format_time(start_time)} --> {format_time(end_time)}")
                srt_content.append(chunk_text)
                srt_content.append("")
            
            # Write SRT file
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(srt_content))
            
            return True
        except Exception as e:
            self.log_console(f"      ‚ùå L·ªói t·∫°o file SRT: {str(e)}")
            return False
    
    def _create_subtitle_clips(self, text, start_offset, end_offset):
        """Create subtitle image clips for MoviePy composite (720p optimized, FAST)"""
        try:
            import textwrap
            
            # Split text into LARGER chunks (60 chars instead of 30) = fewer clips = faster
            words = text.split()
            chunks = []
            current_chunk = []
            
            for word in words:
                candidate = current_chunk + [word]
                if len(' '.join(candidate)) <= 60:  # Doubled from 30
                    current_chunk.append(word)
                else:
                    if current_chunk:
                        chunks.append(' '.join(current_chunk))
                    current_chunk = [word]
            
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            
            # Create subtitle clips
            subtitles = []
            duration = end_offset - start_offset
            duration_per_chunk = duration / len(chunks) if chunks else 0
            
            for i, chunk_text in enumerate(chunks):
                start_time = start_offset + (i * duration_per_chunk)
                end_time = start_offset + ((i + 1) * duration_per_chunk)
                
                # 720p settings
                img_w, img_h = 720, 200
                img = Image.new('RGBA', (img_w, img_h), (0, 0, 0, 0))
                draw = ImageDraw.Draw(img)
                
                # Font
                font = None
                try:
                    font = ImageFont.truetype("arial.ttf", 32)
                except:
                    font = ImageFont.load_default()
                
                lines = textwrap.wrap(chunk_text, width=30)
                line_height = 40
                total_text_h = len(lines) * line_height
                current_y = (img_h - total_text_h) // 2
                
                for line in lines:
                    try:
                        if hasattr(draw, 'textbbox'):
                            bbox = draw.textbbox((0, 0), line, font=font)
                            text_w = bbox[2] - bbox[0]
                        else:
                            text_w, _ = draw.textsize(line, font=font)
                    except:
                        text_w = len(line) * 16
                    
                    x = (img_w - text_w) // 2
                    
                    # Full width black background
                    padding_y = 10
                    draw.rectangle([0, current_y - padding_y, img_w, current_y + line_height + padding_y*0.8], fill=(0, 0, 0, 255))
                    draw.text((x, current_y), line, font=font, fill=(255, 255, 255, 255))
                    current_y += line_height
                
                # Save temp image
                thread_id = threading.get_ident()
                temp_dir = f"temp_subs_{thread_id}"
                os.makedirs(temp_dir, exist_ok=True)
                
                temp_filename = f"{temp_dir}/temp_sub_{int(start_time)}_{i}.png"
                temp_path = os.path.abspath(temp_filename)
                img.save(temp_path)
                
                txt_clip = ImageClip(temp_path).with_duration(end_time - start_time)
                txt_clip = txt_clip.with_position(('center', 0.75), relative=True).with_start(start_time)
                subtitles.append(txt_clip)
            
            return subtitles
        except Exception as e:
            self.log_console(f"      ‚ùå L·ªói t·∫°o subtitle clips: {str(e)}")
            return []

    
    def cleanup_temp_files(self):
        """Delete temporary subtitle files"""
        import shutil
        thread_id = threading.get_ident()
        temp_dir = f"temp_subs_{thread_id}"
        if os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                # self.log_console(f"   üßπ ƒê√£ d·ªçn d·∫πp file t·∫°m")
            except Exception as e:
                print(f"Error cleaning up: {e}")

    def generate_subtitles_whisper(self, video_clip):
        """Generate SRT using Whisper AI - 95%+ accuracy with word-level timestamps"""
        global WHISPER_MODEL
        
        try:
            self.log_console(f"   ü§ñ Whisper AI ƒëang nh·∫≠n di·ªán ({video_clip.duration:.1f}s)...")
            
            if video_clip.audio is None:
                self.log_console(f"   üîá Video kh√¥ng c√≥ audio")
                return None
            
            # Check Whisper availability
            if not WHISPER_AVAILABLE:
                self.log_console(f"   ‚ö†Ô∏è Whisper ch∆∞a c√†i. Chuy·ªÉn sang Google...")
                return self.generate_subtitles(video_clip)
            
            # Check if video has audio
            print(f"   üîç Audio object check: {type(video_clip.audio)}")
            if video_clip.audio is None:
                self.log_console(f"   ‚ö†Ô∏è Video kh√¥ng c√≥ audio (Clip.audio is None). Chuy·ªÉn sang Google...")
                return self.generate_subtitles(video_clip)
            
            # Export audio - OPTIMIZED: Use FFmpeg directly (Instant) instead of MoviePy (Slow)
            thread_id = threading.get_ident()
            audio_path = f"temp_full_audio_{thread_id}.wav"
            
            try:
                print(f"   üíæ Exporting audio (FFmpeg Fast) to: {audio_path}")
                # Use subprocess to run FFmpeg directly for speed
                # ffmpeg -i input -vn -acodec pcm_s16le -ar 16000 -ac 1 output.wav
                if hasattr(video_clip, 'filename') and video_clip.filename:
                    # Best case: we have the source filename, extract directly
                    src_file = video_clip.filename
                    subprocess.run([
                        "ffmpeg", "-y", "-i", src_file,
                        "-ss", str(video_clip.start),
                        "-t", str(video_clip.duration),
                        "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
                        audio_path
                    ], capture_output=True, check=True)
                else:
                    # Fallback: MoviePy if no filename (e.g. composite clip)
                    video_clip.audio.write_audiofile(audio_path, logger=None)
            except Exception as e:
                self.log_console(f"   ‚ö†Ô∏è L·ªói FFmpeg extract ({e}), th·ª≠ export th∆∞·ªùng...")
                try:
                    video_clip.audio.write_audiofile(audio_path, logger=None)
                except Exception as e2:
                     self.log_console(f"   ‚ùå L·ªói export audio: {e2}, chuy·ªÉn sang Google...")
                     return self.generate_subtitles(video_clip)
            except Exception as e:
                self.log_console(f"   ‚ùå L·ªói export audio: {e}, chuy·ªÉn sang Google...")
                return self.generate_subtitles(video_clip)
            
            try:
                # Load Whisper model (only once, thread-safe)
                global WHISPER_MODEL
                if WHISPER_MODEL is None:
                    with WHISPER_SEMAPHORE:
                        if WHISPER_MODEL is None:
                            self.log_console(f"   üì• ƒêang t·∫£i Whisper model (small - nhanh & ch√≠nh x√°c)...")
                            # Use "small" model for speed (base < small < medium < large)
                            # small: ~95% accuracy, 3-4x faster than medium
                            WHISPER_MODEL = whisper.load_model("small")
                
                # Transcribe with optimized parameters
                self.log_console(f"   üéØ ƒêang ph√¢n t√≠ch audio (Whisper Small - 95%+ ch√≠nh x√°c, nhanh)...")
                with WHISPER_SEMAPHORE:
                    # Optimized parameters for SPEED + good accuracy:
                    # - word_timestamps=True: Get word-level timing for better subtitle sync
                    # - temperature=0: Greedy decoding (most accurate, no randomness)
                    # - best_of=1: Faster (was 5)
                    # - beam_size=1: Faster greedy search (was 5)
                    # - condition_on_previous_text=True: Use context from previous segments
                    result = WHISPER_MODEL.transcribe(
                        audio_path,
                        word_timestamps=True,
                        verbose=False,
                        temperature=0,
                        best_of=1,
                        beam_size=1,
                        condition_on_previous_text=True
                    )
                
                # Process segments with improved chunking
                all_segments = []
                total_words = 0
                
                if result and 'segments' in result:
                    for seg in result['segments']:
                        text = seg.get('text', '').strip()
                        if not text: continue
                        
                        words = seg.get('words', [])
                        total_words += len(words)
                        
                        # Split long segments into readable chunks (40 chars max for mobile)
                        if words and len(text) > 40:
                            # Split using word timestamps for perfect sync
                            current_chunk = []
                            chunk_start = None
                            
                            for i, w in enumerate(words):
                                word_text = w.get('word', '').strip()
                                word_start = w.get('start', 0)
                                word_end = w.get('end', 0)
                                
                                if chunk_start is None: chunk_start = word_start
                                
                                # Test if adding this word exceeds 40 chars
                                test_len = len(' '.join(current_chunk + [word_text]))
                                if test_len <= 40:
                                    current_chunk.append(word_text)
                                else:
                                    # Save current chunk
                                    chunk_end = words[i-1]['end'] if i > 0 else word_end
                                    chunk_text = ' '.join(current_chunk)
                                    all_segments.append((chunk_start, chunk_end, chunk_text))
                                    self.log_console(f"   üìù [{chunk_start:.1f}s]: {chunk_text[:30]}...")
                                    
                                    # Start new chunk
                                    current_chunk = [word_text]
                                    chunk_start = word_start
                            
                            # Add last chunk
                            if current_chunk:
                                chunk_text = ' '.join(current_chunk)
                                all_segments.append((chunk_start, word_end, chunk_text))
                                self.log_console(f"   üìù [{chunk_start:.1f}s]: {chunk_text[:30]}...")
                        else:
                            # Short segment, use as is
                            all_segments.append((seg.get('start', 0), seg.get('end', 0), text))
                            self.log_console(f"   üìù [{seg.get('start', 0):.1f}s]: {text[:30]}...")
                            
                # Create SRT file in dedicated folder
                if all_segments:
                    # Create srt_files directory if not exists
                    srt_dir = "srt_files"
                    os.makedirs(srt_dir, exist_ok=True)
                    
                    srt_path = os.path.join(srt_dir, f"temp_subs_{thread_id}.srt")
                    
                    # Get speed factor from settings
                    speed_factor = self.speed_factor.get()
                    
                    with open(srt_path, 'w', encoding='utf-8') as f:
                        for idx, (start, end, text) in enumerate(all_segments, 1):
                            # Adjust timing for video speed (divide by speed_factor)
                            start_adjusted = start / speed_factor
                            end_adjusted = end / speed_factor
                            
                            h = int(start_adjusted // 3600)
                            m = int((start_adjusted % 3600) // 60)
                            s = int(start_adjusted % 60)
                            ms = int((start_adjusted % 1) * 1000)
                            h2 = int(end_adjusted // 3600)
                            m2 = int((end_adjusted % 3600) // 60)
                            s2 = int(end_adjusted % 60)
                            ms2 = int((end_adjusted % 1) * 1000)
                            f.write(f"{idx}\n")
                            f.write(f"{h:02d}:{m:02d}:{s:02d},{ms:03d} --> {h2:02d}:{m2:02d}:{s2:02d},{ms2:03d}\n")
                            f.write(f"{text}\n\n")
                    
                    self.log_console(f"   ‚úÖ SRT: {len(all_segments)} ƒëo·∫°n, {total_words} t·ª´ (Whisper Small - 95%+ ch√≠nh x√°c, nhanh)")
                    self.log_console(f"   üéØ Ng√¥n ng·ªØ: {result.get('language', 'unknown')}")
                    return srt_path
                
                return None

            except Exception as e:
                self.log_console(f"   ‚ùå L·ªói Whisper: {e}, chuy·ªÉn sang Google...")
                return self.generate_subtitles(video_clip)
                
            finally:
                # Always cleanup audio file
                if os.path.exists(audio_path):
                    try:
                        os.remove(audio_path)
                    except:
                        pass
                        
        except Exception as e:
            self.log_console(f"   ‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh Whisper: {e}")
            return self.generate_subtitles(video_clip)

    def generate_subtitles(self, video_clip):
        """Generate SRT subtitle file - Split into chunks for full coverage"""
        if not self.recognizer:
            self.log_console(f"   ‚ö†Ô∏è Speech Recognizer ch∆∞a s·∫µn s√†ng")
            return None
        
        try:
            self.log_console(f"   üé§ ƒêang nh·∫≠n di·ªán gi·ªçng n√≥i ({video_clip.duration:.1f}s)...")
            
            if video_clip.audio is None:
                self.log_console(f"   üîá Video kh√¥ng c√≥ audio")
                return None
            
            # Split audio into 8-second chunks for better timing and accuracy
            chunk_duration = 8
            all_text_segments = []
            total_duration = video_clip.duration
            
            for chunk_start in range(0, int(total_duration), chunk_duration):
                chunk_end = min(chunk_start + chunk_duration, total_duration)
                
                try:
                    audio_chunk = video_clip.audio.subclipped(chunk_start, chunk_end)
                    chunk_audio_raw = f"temp_audio_raw_{chunk_start}.wav"
                    chunk_audio_path = f"temp_audio_chunk_{chunk_start}.wav"
                    
                    # Export raw audio
                    audio_chunk.write_audiofile(chunk_audio_raw, logger=None)
                    
                    # Apply audio filtering with FFmpeg for better speech recognition
                    try:
                        from imageio_ffmpeg import get_ffmpeg_exe
                        ffmpeg_path = get_ffmpeg_exe()
                    except:
                        ffmpeg_path = 'ffmpeg'
                    
                    # Audio filters for speech enhancement:
                    # - highpass=200: Remove low-frequency noise
                    # - lowpass=3000: Focus on speech frequencies
                    # - volume=2.0: Boost volume for better recognition
                    import subprocess
                    filter_cmd = [
                        ffmpeg_path, '-y', '-i', chunk_audio_raw,
                        '-af', 'highpass=f=200,lowpass=f=3000,volume=2.0',
                        '-ar', '16000',  # 16kHz sample rate (optimal for speech)
                        '-ac', '1',      # Mono
                        chunk_audio_path
                    ]
                    subprocess.run(filter_cmd, capture_output=True, text=True)
                    
                    # Clean up raw file
                    if os.path.exists(chunk_audio_raw):
                        os.remove(chunk_audio_raw)
                    
                    with sr.AudioFile(chunk_audio_path) as source:
                        # Adjust for ambient noise
                        self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                        audio_data = self.recognizer.record(source)
                    
                    text = None
                    try:
                        text = self.recognizer.recognize_google(audio_data, language="en-US")
                    except sr.UnknownValueError:
                        try:
                            text = self.recognizer.recognize_google(audio_data, language="vi-VN")
                        except:
                            pass
                    except:
                        pass
                    
                    if os.path.exists(chunk_audio_path):
                        os.remove(chunk_audio_path)
                    
                    if text:
                        # Only skip obvious noise, keep all real speech (even short phrases)
                        skip_phrases = ['ok ok', 'okay okay', 'uh uh', 'um um', 'mm mm']
                        if any(phrase == text.lower().strip() for phrase in skip_phrases):
                            continue
                        
                        all_text_segments.append((chunk_start, chunk_end, text))
                        self.log_console(f"   üìù [{chunk_start}s-{chunk_end:.0f}s]: {text[:50]}...")
                except:
                    continue
            
            if all_text_segments:
                thread_id = threading.get_ident()
                
                # Create srt_files directory if not exists
                srt_dir = "srt_files"
                os.makedirs(srt_dir, exist_ok=True)
                
                srt_path = os.path.join(srt_dir, f"temp_subs_{thread_id}.srt")
                
                # Use full segments (no chunking) - keeps natural sentence flow
                with open(srt_path, 'w', encoding='utf-8') as f:
                    for idx, (start, end, text) in enumerate(all_text_segments, 1):
                        def format_time(seconds):
                            hours = int(seconds // 3600)
                            minutes = int((seconds % 3600) // 60)
                            secs = int(seconds % 60)
                            millis = int((seconds % 1) * 1000)
                            return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"
                        
                        f.write(f"{idx}\n")
                        f.write(f"{format_time(start)} --> {format_time(end)}\n")
                        f.write(f"{text}\n\n")
                
                self.log_console(f"   ‚úÖ T·∫°o file SRT v·ªõi {len(all_text_segments)} ƒëo·∫°n")
                return srt_path
            else:
                return None
        except Exception as e:
            self.log_console(f"   ‚ùå L·ªói: {str(e)}")
            return None

    def process_video_with_ffmpeg(self, input_path, output_path, srt_file=None):
        """Process video entirely with FFmpeg - ULTRA FAST"""
        try:
            from imageio_ffmpeg import get_ffmpeg_exe
            ffmpeg_path = get_ffmpeg_exe()
        except:
            ffmpeg_path = 'ffmpeg'
        
        # Build FFmpeg filter chain
        filters = []
        
        # 1. Trim video
        start_t = self.start_time.get()
        duration = self.duration.get()
        
        # 2. Speed
        speed_val = self.speed_factor.get()
        if speed_val != 1.0:
            filters.append(f"setpts={1/speed_val}*PTS")
        
        # 3. Mirror (horizontal flip)
        if self.mirror_enabled.get():
            filters.append("hflip")
        
        # 4. Black & White
        if self.blackwhite_enabled.get():
            filters.append("hue=s=0")
        
        # 5. Saturation
        sat_val = self.saturation.get()
        if sat_val != 1.0:
            filters.append(f"eq=saturation={sat_val}")
        
        # 6. Brightness
        bright_val = self.brightness.get()
        if bright_val != 1.0:
            filters.append(f"eq=brightness={bright_val-1.0}")  # FFmpeg uses -1 to 1, we use 0 to 2
        
        # üé® NEW: Advanced Color Grading
        # Contrast
        contrast_val = self.contrast.get()
        if contrast_val != 1.0:
            filters.append(f"eq=contrast={contrast_val}")
        
        # Vibrance (using saturation + selective color boost)
        vibrance_val = self.vibrance.get()
        if vibrance_val != 0:
            # Vibrance affects less-saturated colors more
            vib_factor = 1.0 + (vibrance_val / 100.0)
            filters.append(f"eq=saturation={vib_factor}")
        
        # Color Filters
        color_filter = self.color_filter.get()
        if color_filter == "Vintage":
            filters.append("curves=vintage")
            filters.append("eq=contrast=1.1:brightness=0.05")
        elif color_filter == "Cinematic":
            filters.append("curves=preset=color_negative")
            filters.append("eq=contrast=1.2:saturation=0.9")
        elif color_filter == "Cool":
            filters.append("colorbalance=rs=-0.2:gs=0:bs=0.2")
        elif color_filter == "Warm":
            filters.append("colorbalance=rs=0.2:gs=0.1:bs=-0.2")
        
        # üåü NEW: Visual Effects
        # Vignette
        if self.vignette_enabled.get():
            strength = self.vignette_strength.get()
            filters.append(f"vignette=angle=PI/4:mode=forward:eval=frame:dither=1:aspect=16/9:x0=0.5:y0=0.5")
        
        # Sharpen
        sharpen_val = self.sharpen_amount.get()
        if sharpen_val > 0:
            filters.append(f"unsharp=5:5:{sharpen_val}:5:5:0")
        
        # üîÑ NEW: Transform (Rotation removed - caused issues)
        # Flip only
        if self.flip_horizontal.get():
            filters.append("hflip")
        if self.flip_vertical.get():
            filters.append("vflip")
        
        # üîÑ NEW: Auto Rotate Landscape to Portrait (90¬∞ clockwise)
        if self.rotate_to_portrait.get():
            filters.append("transpose=1")  # 1 = 90¬∞ clockwise (ngang ‚Üí d·ªçc)
            self.log_console("   üîÑ Xoay video 90¬∞ (Ngang ‚Üí D·ªçc)")
        
        # ‚è±Ô∏è NEW: Timing Effects (will be added later in complex filter)
        # Fade effects need duration info, will add after getting video duration
        
        # 6. Portrait mode (720x1280) with MAIN VIDEO stretched to 230% height
        use_complex_filter = False
        complex_filter = None
        
        # üü¢ MODE 1: SIMPLE MODE (Ch·ªâ th√™m Subtitle - Gi·ªØ nguy√™n video g·ªëc)
        if self.simple_mode.get():
            # Keep original video as-is (no crop, no blur, no effects)
            # Only add subtitle if enabled
            
            # Apply basic filters only (speed, mirror if enabled)
            clean_filters = []
            for f in filters:
                if any(x in f for x in ['trim', 'setpts', 'hflip', 'vflip', 'atempo']):
                    clean_filters.append(f)
            
            vf = ','.join(clean_filters) if clean_filters else None
            use_complex_filter = False
            complex_filter = None
            final_output = None
            
            self.log_console(f"   ‚ÑπÔ∏è Simple Mode: Gi·ªØ nguy√™n video g·ªëc + Th√™m subtitle")


        # üü¢ MODE 2: PORTRAIT MODE (Full Effects)
        elif self.convert_to_portrait.get():
            # Target frame: 1080x1920 (1080p portrait for phones)
            target_w = 1080
            target_h = 1920
            
            # Build base filters (speed, mirror, effects)
            base_filters = ','.join(filters) if filters else 'null'
            
            # Complex filter strategy - RESTORED 230% STRETCH:
            # Stretch video height to 230% of original aspect ratio width, centered
            complex_filter = (
                f"[0:v]{base_filters}[base];"
                f"[base]split[bg][main];"
                # Background: scale to fill frame + blur
                f"[bg]scale={target_w}:{target_h}:force_original_aspect_ratio=increase,"
                f"crop={target_w}:{target_h},"
                # Optimized Blur: boxblur is much faster than gblur
                f"boxblur=20:2[blurred];"
                # Main video: 
                # 1. Scale width to 720, height proportional (mult of 2)
                # 2. Stretch height to 230% (distort)
                f"[main]scale={target_w}:-2[scaled];"
                f"[scaled]scale=iw:trunc(ih*2.3/2)*2[stretched];"
                # Overlay: blur background + stretched video (centered)
                # overlay=(W-w)/2:(H-h)/2 handles both smaller and larger overlays correctly
                f"[blurred][stretched]overlay=(W-w)/2:(H-h)/2[video_with_bg];"
                # Add LIVE badge (blinking red badge in top-left corner)
                f"[video_with_bg]drawtext="
                f"text='LIVE':"
                f"fontfile=/Windows/Fonts/arialbd.ttf:"
                f"fontsize=28:"
                f"fontcolor=white:"
                f"box=1:"
                f"boxcolor=red@0.8:"
                f"boxborderw=8:"
                f"x=20:"
                f"y=20:"
                f"enable='mod(floor(t),2)'"  # Blink every second
            )
            
            # Always draw black box to cover original subtitles (bottom 450px, fully opaque)
            complex_filter += (
                f"[out];"
                f"[out]drawbox=0:ih-380:iw:380:color=black@1.0:t=fill[covered]"
            )
            
            # Add our new subtitles on top if available
            if srt_file:
                srt_path_escaped = os.path.abspath(srt_file).replace('\\', '/').replace(':', '\\:')
                
                # Get subtitle customization settings
                font_size = self.subtitle_font_size.get()
                outline_width = self.subtitle_outline.get()
                
                # Convert color name to hex for FFmpeg
                color_map = {
                    "white": "&HFFFFFF",
                    "yellow": "&H00FFFF",
                    "cyan": "&HFFFF00",
                    "red": "&H0000FF",
                    "green": "&H00FF00"
                }
                primary_color = color_map.get(self.subtitle_color.get(), "&HFFFFFF")
                
                subtitle_style = (
                    "force_style='"
                    "FontName=Arial Black,"
                    f"FontSize={font_size},"  # Customizable
                    f"PrimaryColour={primary_color},"  # Customizable color
                    "OutlineColour=&H000000,"  # Black outline
                    f"Outline={outline_width},"  # Customizable outline
                    "Bold=1,"
                    "Alignment=2,"  # Bottom center
                    "MarginV=40"  # 40px from bottom (lower position)
                    "'"
                )
                complex_filter += f";[covered]subtitles='{srt_path_escaped}':{subtitle_style}[with_subs]"
                final_output = "[with_subs]"
            else:
                final_output = "[covered]"
            
            # üí¨ NEW: Add Watermark
            watermark_text = self.watermark_text.get()
            if watermark_text:
                wm_pos = self.watermark_position.get()
                wm_opacity = self.watermark_opacity.get()
                wm_size = self.watermark_fontsize.get()
                wm_color = self.watermark_color.get()
                
                # Calculate position
                if wm_pos == "top-left":
                    x, y = "20", "20"
                elif wm_pos == "top-right":
                    x, y = "w-tw-20", "20"
                elif wm_pos == "bottom-left":
                    x, y = "20", "h-th-20"
                elif wm_pos == "bottom-right":
                    x, y = "w-tw-20", "h-th-20"
                else:  # center
                    x, y = "(w-tw)/2", "(h-th)/2"
                
                complex_filter += (
                    f";{final_output}drawtext="
                    f"text='{watermark_text}':"
                    f"fontfile=/Windows/Fonts/arial.ttf:"
                    f"fontsize={wm_size}:"
                    f"fontcolor={wm_color}@{wm_opacity}:"
                    f"x={x}:y={y}[with_wm]"
                )
                final_output = "[with_wm]"
            
            # ‚è±Ô∏è NEW: Add Fade Effects
            fade_in = self.fade_in_duration.get()
            fade_out = self.fade_out_duration.get()
            
            if fade_in > 0 or fade_out > 0:
                fade_filters = []
                if fade_in > 0:
                    fade_filters.append(f"fade=t=in:st=0:d={fade_in}")
                if fade_out > 0:
                    # Fade out at end (duration - fade_out)
                    dur = self.duration.get()
                    fade_filters.append(f"fade=t=out:st={dur-fade_out}:d={fade_out}")
                
                if fade_filters:
                    complex_filter += f";{final_output}{','.join(fade_filters)}[faded]"
                    final_output = "[faded]"
            
            # üëá NEW: Pointing Hand Sticker (Blinking)
            if self.enable_pointing_hand.get():
                sticker_path = os.path.abspath("assets/pointing_hand.png")
                if os.path.exists(sticker_path):
                    # Escape path for FFmpeg
                    sticker_escaped = sticker_path.replace('\\', '/').replace(':', '\\:')
                    
                    # Overlay sticker at bottom RIGHT with blinking effect
                    # Size: 15% of video width
                    # Position: Right side (60% from left), 5% from bottom (lower)
                    # Blink: fade in/out every 0.5 seconds
                    complex_filter += (
                        f";movie='{sticker_escaped}',scale=iw*0.15:-1[sticker];"
                        f"{final_output}[sticker]overlay="
                        f"x=W*0.75:"  # 75% from left (moved more to right)
                        f"y=H-h-H*0.05:"  # 5% from bottom (lower)
                        f"enable='mod(floor(t*2),2)'"  # Blink every 0.5s
                        f"[with_sticker]"
                    )
                    final_output = "[with_sticker]"
                    self.log_console(f"   üëá ƒê√£ th√™m sticker ng√≥n tay ch·ªâ xu·ªëng")
                else:
                    self.log_console(f"   ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y sticker: {sticker_path}")
            
            
            
            # üé¨ NEW: Outro (Disable for Simple Mode)
            if not self.simple_mode.get():
                dur = self.duration.get()
                outro_duration = 5
                outro_start = dur - outro_duration
                
                # Simple fade to black + text overlay
                complex_filter += (
                    f";{final_output}fade=t=out:st={outro_start}:d=1.5[faded_out];"
                    f"[faded_out]drawtext="
                    f"text='Please wait for Part ':"
                    f"fontfile=/Windows/Fonts/arialbd.ttf:"
                    f"fontsize=40:"
                    f"fontcolor=white:"
                    f"x=(w-tw)/2:"
                    f"y=(h-th)/2:"
                    f"enable='gte(t,{outro_start})'[end]"
                )
                final_output = "[end]"
            
            use_complex_filter = True
            filters = []  # Clear simple filters
        else:
            # Non-portrait mode: use simple filters
            filters.append("scale=1080:-1:force_original_aspect_ratio=decrease")
            
            # Always add black box to cover original subtitles (450px, fully opaque)
            filters.append("drawbox=0:ih-380:iw:380:color=black@1.0:t=fill")
            
            # Add our new subtitles on top if available
            if srt_file:
                srt_path_escaped = os.path.abspath(srt_file).replace('\\', '/').replace(':', '\\:')
                
                subtitle_style = (
                    "force_style='"
                    "FontName=Arial Black,"
                    "FontSize=14,"  # Smaller, max 2 lines
                    "PrimaryColour=&HFFFFFF,"  # White text
                    "OutlineColour=&H000000,"  # Black outline
                    "Outline=3,"
                    "Bold=1,"
                    "Alignment=2,"  # Bottom center
                    "MarginV=40"  # 40px from bottom (lower position)
                    "'"
                )
                filters.append(f"subtitles='{srt_path_escaped}':{subtitle_style}")
        
        # Combine simple filters
        vf = ','.join(filters) if filters and not use_complex_filter else None
        
        # Audio filters
        audio_filters = []
        
        # Volume boost
        vol_val = self.volume_boost.get()
        if vol_val != 1.0:
            audio_filters.append(f"volume={vol_val}")
        
        # Speed (audio)
        if speed_val != 1.0:
            audio_filters.append(f"atempo={speed_val}")
        
        # üîä NEW: Audio Effects
        # Echo
        if self.audio_echo.get():
            audio_filters.append("aecho=0.8:0.9:1000:0.3")
        
        # Bass Boost
        bass_boost = self.bass_boost.get()
        if bass_boost > 0:
            audio_filters.append(f"bass=g={bass_boost}")
        
        # Treble Boost
        treble_boost = self.treble_boost.get()
        if treble_boost > 0:
            audio_filters.append(f"treble=g={treble_boost}")
        
        af = ','.join(audio_filters) if audio_filters else None
        
        # Build FFmpeg command
        cmd = [ffmpeg_path, '-y']
        
        # Add HW Acceleration for decoding if GPU enabled
        if hasattr(self, 'use_gpu') and self.use_gpu.get():
            cmd.extend(['-hwaccel', 'cuda'])
        
        # üöÄ MULTI-THREADING: Enable parallel filter processing
        # -threads 0: Auto-detect CPU cores for encoding
        # -filter_threads 0: Auto-detect CPU cores for filter processing (blur, scale, etc.)
        cmd.extend([
            '-threads', '0',
            '-filter_threads', '0'
        ])
            
        cmd.extend([
            '-ss', str(start_t),
            '-t', str(duration),
            '-i', input_path
        ])
        
        # Use filter_complex for portrait mode with blur, or simple -vf for others
        if use_complex_filter and complex_filter:
            cmd.extend(['-filter_complex', complex_filter])
            # Explicitly map the complex filter output and original audio
            cmd.extend(['-map', final_output])  # Video from filter
            cmd.extend(['-map', '0:a'])         # Audio from input 0
        elif vf:
            cmd.extend(['-vf', vf])
            # No map needed, ffmpeg auto-selects
            
        if af:
            cmd.extend(['-af', af])
        
        # Encoding Settings - OPTIMIZED FOR SPEED
        if hasattr(self, 'use_gpu') and self.use_gpu.get():
            cmd.extend([
                '-c:v', 'h264_nvenc',  # GPU Encoder
                '-preset', 'p1',       # P1 = FASTEST preset (was p2)
                '-rc', 'vbr',          # Variable Bitrate
                '-cq', '28',           # Higher CQ = faster encode (was 23)
                '-b:v', '3000k',       # Set max bitrate for speed
                '-maxrate', '4000k',
                '-bufsize', '8000k'
            ])
        else:
            cmd.extend([
                '-c:v', 'libx264',     # CPU Encoder
                '-preset', 'ultrafast',
                '-crf', '28',          # Higher = faster
            ])
            
        cmd.extend([
            '-pix_fmt', 'yuv420p',  # CRITICAL: Ensures video plays on Windows/Phone
            '-r', '30',  # 30 FPS for faster encoding (was 60)
            '-c:a', 'aac',
            '-b:a', '128k',
            output_path
        ])
        
        self.log_console(f"   üöÄ FFmpeg processing...")
        self.log_console(f"   üîß Command: {' '.join(cmd)}")
        
        import subprocess
        
        # Use GPU semaphore to limit concurrent GPU encodes
        use_gpu = hasattr(self, 'use_gpu') and self.use_gpu.get()
        
        if use_gpu:
            # Acquire GPU slot (max 2 concurrent)
            GPU_ENCODE_SEMAPHORE.acquire()
            self.log_console(f"   üéÆ GPU slot acquired")
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8', errors='ignore')
        finally:
            if use_gpu:
                GPU_ENCODE_SEMAPHORE.release()
                self.log_console(f"   üéÆ GPU slot released")
        
        if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
            return True
        else:
            self.log_console(f"   ‚ùå FFmpeg failed!")
            self.log_console(f"   üìã STDERR: {result.stderr[-1000:]}")  # Last 1000 chars
            return False

    def process_single_video(self, filename):
        start_time = time.time()
        try:
            input_dir = self.input_dir.get()
            output_dir = self.output_dir.get()
            duration = self.duration.get()

            os.makedirs(output_dir, exist_ok=True)
            input_path = os.path.join(input_dir, filename)
            output_path = os.path.join(output_dir, filename)

            # Add to tree
            self.add_video_to_tree(filename)
            
            # Log thread info for debugging parallel execution
            thread_id = threading.current_thread().name
            self.log_console(f"üé¨ [{thread_id}] Starting: {filename}")
            
            self.update_video_status(filename, "üöÄ Processing...")

            # Step 1: Generate subtitles if needed (only load needed portion)
            srt_file = None
            if self.enable_subtitles.get() and self.recognizer and not self.simple_mode.get():
                self.update_video_status(filename, "üé§ Generating Subtitles...")
                
                # Check AI engine selection
                selected_ai = self.ai_engine.get()
                
                # Only load the portion we need (start_time to start_time+duration)
                start_t = self.start_time.get()
                dur = self.duration.get()
                
                video_temp = VideoFileClip(input_path).subclip(start_t, start_t + dur)
                
                # Use selected AI engine
                if selected_ai == "Whisper AI" and WHISPER_AVAILABLE:
                    srt_file = self.generate_subtitles_whisper(video_temp)
                elif selected_ai == "Google Speech" and SPEECH_RECOGNITION_AVAILABLE:
                    srt_file = self.generate_subtitles(video_temp)
                else:
                    # Fallback
                    if WHISPER_AVAILABLE:
                        srt_file = self.generate_subtitles_whisper(video_temp)
                    elif SPEECH_RECOGNITION_AVAILABLE:
                        srt_file = self.generate_subtitles(video_temp)
                
                video_temp.close()
                
                elapsed = int(time.time() - start_time)
                if srt_file:
                    self.update_video_status(filename, "‚úÖ Subtitles Ready", "", f"{elapsed}s")
            
            # Step 2: Process video with FFmpeg (ULTRA FAST!)
            self.update_video_status(filename, "üé¨ Encoding Video...")
            success = self.process_video_with_ffmpeg(input_path, output_path, srt_file)
            
            if success:
                # Get ffmpeg path
                import subprocess
                try:
                    import imageio_ffmpeg
                    ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()
                except:
                    ffmpeg_path = 'ffmpeg'
                
                # üé¨ INTRO/OUTRO Concatenation (NEW)
                intro_path = self.intro_video_path.get() if self.enable_intro.get() else ""
                outro_path = self.outro_video_path.get() if self.enable_outro.get() else ""
                
                # DEBUG: Log intro/outro status
                self.log_console(f"   üîç DEBUG - Enable Intro: {self.enable_intro.get()}, Path: {self.intro_video_path.get()}")
                self.log_console(f"   üîç DEBUG - Enable Outro: {self.enable_outro.get()}, Path: {self.outro_video_path.get()}")
                self.log_console(f"   üîç DEBUG - Simple Mode: {self.simple_mode.get()}")
                
                # Check if we need to concatenate
                need_concat = (intro_path and os.path.exists(intro_path)) or (outro_path and os.path.exists(outro_path))
                
                self.log_console(f"   üîç DEBUG - Need Concat: {need_concat}")
                
                if need_concat and not self.simple_mode.get():
                    self.update_video_status(filename, "üéûÔ∏è Gh√©p Intro/Outro...")
                    try:
                        # Rename main output to temp (with retry for file lock issues)
                        # Use UUID to prevent collision in multi-thread
                        main_temp = output_path.replace(".mp4", f"_temp_{uuid.uuid4().hex[:8]}.mp4")
                        
                        # Wait a bit for FFmpeg to release the file
                        # Wait a bit for FFmpeg to release the file
                        time.sleep(0.5)
                        
                        # Remove old temp file if exists (with retry)
                        if os.path.exists(main_temp):
                            for retry in range(3):
                                try:
                                    os.remove(main_temp)
                                    break
                                except PermissionError:
                                    if retry < 2:
                                        time.sleep(0.5)
                                    else:
                                        raise
                        
                        # Rename with retry
                        for retry in range(3):
                            try:
                                os.rename(output_path, main_temp)
                                break
                            except PermissionError:
                                if retry < 2:
                                    time.sleep(0.5)
                                else:
                                    raise
                        
                        
                        # Build concat list
                        videos_to_concat = []
                        if intro_path and os.path.exists(intro_path):
                            videos_to_concat.append(("intro", os.path.abspath(intro_path)))
                            self.log_console(f"   üé• Intro: {os.path.basename(intro_path)}")
                        
                        videos_to_concat.append(("main", main_temp))
                        
                        if outro_path and os.path.exists(outro_path):
                            videos_to_concat.append(("outro", os.path.abspath(outro_path)))
                            self.log_console(f"   üé¨ Outro: {os.path.basename(outro_path)}")
                        
                        # Get codec and preset
                        # FORCE CPU for concatenation for maximum stability
                        # NVENC often fails with "Invalid Argument" when concatenating different videos
                        codec = 'libx264'
                        preset = 'ultrafast'
                        
                        # Build FFmpeg filter_complex for concatenation
                        # Scale all videos to FIX RESOLUTION (avoid odd sizes / scale2ref crashes)
                        
                        # Determine Target Resolution based on Mode
                        # Default is Portrait 9:16
                        tgt_w, tgt_h = 1080, 1920
                        
                        # Check if we are in Landscape/Simple mode
                        # (If simple_mode is ON, or convert_to_portrait is OFF)
                        # Accessing UI variables from thread needs care, but .get() is usually fine or we check internal flags
                        is_portrait = True
                        if hasattr(self, 'simple_mode') and self.simple_mode.get():
                            is_portrait = False
                            tgt_w, tgt_h = 1920, 1080
                        elif hasattr(self, 'convert_to_portrait') and not self.convert_to_portrait.get():
                            is_portrait = False
                            tgt_w, tgt_h = 1920, 1080
                            
                        # Universal Scale Filter: Fit to box, Pad with black, Force Even dimensions, Force SAR 1
                        scale_filter = (
                            f"scale={tgt_w}:{tgt_h}:force_original_aspect_ratio=decrease,"
                            f"pad={tgt_w}:{tgt_h}:(ow-iw)/2:(oh-ih)/2,"
                            f"setsar=1"
                        )
                        
                        # Helper to check audio stream
                        def has_audio_stream(fpath):
                            try:
                                # Quick probe
                                r = subprocess.run([ffmpeg_path, '-i', fpath], capture_output=True, text=True, encoding='utf-8', errors='ignore')
                                return "Audio:" in r.stderr
                            except:
                                return False

                        # Build Dynamic Filter Complex
                        filter_parts = []
                        concat_inputs = []
                        
                        # Input args
                        cmd_inputs = []
                        for idx, (vtype, vpath) in enumerate(videos_to_concat):
                            cmd_inputs.extend(['-i', vpath])
                            
                            # Video Scale
                            filter_parts.append(f"[{idx}:v]{scale_filter}[v{idx}];")
                            
                            # Audio Handle (Check existing or generate silence)
                            if has_audio_stream(vpath):
                                filter_parts.append(f"[{idx}:a]aresample=44100,aformat=channel_layouts=stereo[a{idx}];")
                            else:
                                self.log_console(f"   ‚ö†Ô∏è Video '{vtype}' kh√¥ng c√≥ audio, th√™m silence...")
                                filter_parts.append(f"anullsrc=r=44100:cl=stereo[a{idx}];")
                            
                            concat_inputs.append(f"[v{idx}][a{idx}]")
                        
                        # Concat command
                        concat_segment = "".join(concat_inputs)
                        filter_parts.append(f"{concat_segment}concat=n={len(videos_to_concat)}:v=1:a=1[outv][outa]")
                        
                        full_filter_complex = "".join(filter_parts)
                        
                        cmd_concat = [ffmpeg_path, '-y'] + cmd_inputs + [
                            '-filter_complex', full_filter_complex,
                            '-map', '[outv]',
                            '-map', '[outa]',
                            '-c:v', codec, '-preset', preset, '-pix_fmt', 'yuv420p',
                            '-c:a', 'aac',
                            output_path
                        ]
                        
                        self.log_console(f"   üîß Gh√©p {len(videos_to_concat)} video...")
                        result = subprocess.run(cmd_concat, capture_output=True, text=True, encoding='utf-8', errors='ignore')
                        
                        if result.returncode != 0:
                            self.log_console(f"   ‚ùå Gh√©p th·∫•t b·∫°i! Return code: {result.returncode}")
                            self.log_console(f"   üìã STDERR: {result.stderr[-500:]}")
                            # Restore main file
                            if os.path.exists(main_temp):
                                if os.path.exists(output_path):
                                    try:
                                        os.remove(output_path)
                                    except:
                                        pass
                                os.replace(main_temp, output_path)
                                self.log_console(f"   ‚úÖ Kh√¥i ph·ª•c video g·ªëc")
                        else:
                            # Success, cleanup temp
                            if os.path.exists(main_temp):
                                os.remove(main_temp)
                            self.log_console(f"   ‚úÖ Gh√©p Intro/Outro th√†nh c√¥ng!")
                            
                    except Exception as e:
                        error_msg = f"‚ùå L·ªói gh√©p Intro/Outro: {e}"
                        print(error_msg)
                        self.log_console(error_msg)
                        # Restore main file if failed
                        if os.path.exists(main_temp):
                            if os.path.exists(output_path):
                                try:
                                    os.remove(output_path)
                                except:
                                    pass
                            try:
                                os.replace(main_temp, output_path)
                                self.log_console(f"   ‚úÖ Kh√¥i ph·ª•c video g·ªëc")
                            except:
                                pass
                
                
                elapsed = int(time.time() - start_time)
                self.update_video_status(filename, "‚úÖ Completed", "100%", f"{elapsed}s")
                
                # Cleanup SRT file
                if srt_file and os.path.exists(srt_file):
                    os.remove(srt_file)
                
                return True
            else:
                elapsed = int(time.time() - start_time)
                self.update_video_status(filename, "‚ùå Failed", "0%", f"{elapsed}s")
                return False


        except Exception as e:
            elapsed = int(time.time() - start_time)
            self.update_video_status(filename, f"‚ùå Error: {str(e)[:30]}", "0%", f"{elapsed}s")
            return False


    def get_unprocessed_videos(self):
        input_dir = self.input_dir.get()
        output_dir = self.output_dir.get()
        if not os.path.exists(input_dir):
            return []
        all_videos = [f for f in os.listdir(input_dir) if f.lower().endswith((".mp4", ".avi", ".mov", ".mkv"))]
        unprocessed = []
        for video in all_videos:
            output_path = os.path.join(output_dir, video)
            if video not in self.processed_files and not os.path.exists(output_path):
                unprocessed.append(video)
        return unprocessed

    def auto_watch_folder(self):
        while self.auto_running:
            try:
                unprocessed = self.get_unprocessed_videos()
                if unprocessed:
                    self.log_console(f"üîç Ph√°t hi·ªán {len(unprocessed)} video m·ªõi!")
                    for video_file in unprocessed:
                        if not self.auto_running:
                            break
                        self.process_single_video(video_file)
                        self.processed_files.add(video_file)
                time.sleep(self.watch_interval.get())
            except Exception as e:
                self.log_console(f"‚ùå L·ªói auto-watch: {str(e)}")
                time.sleep(self.watch_interval.get())

    def start_auto_watch(self):
        if not self.auto_running:
            self.auto_running = True
            self.auto_thread = threading.Thread(target=self.auto_watch_folder, daemon=True)
            self.auto_thread.start()

    def stop_auto_watch(self):
        self.auto_running = False

    def toggle_auto_mode(self):
        if self.auto_mode.get():
            self.auto_status_label.config(text="B·∫¨T", fg="#00ff88")
            self.log_console("ü§ñ Ch·∫ø ƒë·ªô t·ª± ƒë·ªông: B·∫¨T")
            self.log_console(f"‚è∞ Ki·ªÉm tra th∆∞ m·ª•c m·ªói {self.watch_interval.get()} gi√¢y\n")
            self.start_auto_watch()
        else:
            self.auto_status_label.config(text="T·∫ÆT", fg="#ff4444")
            self.log_console("ü§ñ Ch·∫ø ƒë·ªô t·ª± ƒë·ªông: T·∫ÆT\n")
            self.stop_auto_watch()

    def process_videos(self):
        try:
            input_dir = self.input_dir.get()
            output_dir = self.output_dir.get()

            if not os.path.exists(input_dir):
                messagebox.showerror("L·ªói", f"Th∆∞ m·ª•c input kh√¥ng t·ªìn t·∫°i!")
                return

            video_files = [f for f in os.listdir(input_dir) if f.lower().endswith((".mp4", ".avi", ".mov", ".mkv"))]
            if not video_files:
                messagebox.showwarning("C·∫£nh b√°o", "Kh√¥ng t√¨m th·∫•y video n√†o!")
                return

            total_videos = len(video_files)
            completed_videos = 0
            
            # Clear tree and reset
            for item in self.video_tree.get_children():
                self.video_tree.delete(item)
            self.video_items.clear()
            
            # Initialize progress
            self.update_overall_progress(0, total_videos)
            
            # Determine threading mode
            # OPTIMIZATION: Always use user-defined thread count!
            max_workers = self.num_threads.get()
            
            if self.enable_subtitles.get():
               self.log_console(f"üöÄ Subtitle B·∫¨T -> Ch·∫°y ƒëa lu·ªìng ({max_workers}) - T·∫≠n d·ª•ng t·ªëi ƒëa CPU/GPU!")
            else:
               self.log_console(f"‚ö° Subtitle T·∫ÆT -> Ch·∫°y ƒëa lu·ªìng ({max_workers}) si√™u t·ªëc!")
            
            self.log_console(f"\nüöÄ Starting parallel processing with {max_workers} threads...")
            self.log_console(f"üìä Total videos to process: {total_videos}\n")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(self.process_single_video, filename): filename for filename in video_files}
                self.log_console(f"‚úÖ Submitted {len(futures)} tasks to thread pool\n")
                
                for future in concurrent.futures.as_completed(futures):
                    filename = futures[future]
                    try:
                        result = future.result()
                        if result:
                            completed_videos += 1
                        self.update_overall_progress(completed_videos, total_videos)
                    except Exception as e:
                        error_msg = f"‚ùå L·ªói ngo·∫°i l·ªá {filename}: {e}"
                        print(error_msg)
                        self.root.after(0, lambda m=error_msg: self.log_console(m))

            # Final update
            self.update_overall_progress(total_videos, total_videos)
            messagebox.showinfo("Th√†nh c√¥ng", f"ƒê√£ x·ª≠ l√Ω xong {completed_videos}/{total_videos} video!")
        except Exception as e:
            messagebox.showerror("L·ªói", f"ƒê√£ x·∫£y ra l·ªói: {str(e)}")
        finally:
            self.is_processing = False


    def start_processing(self):
        if self.is_processing:
            messagebox.showwarning("C·∫£nh b√°o", "ƒêang x·ª≠ l√Ω video, vui l√≤ng ƒë·ª£i!")
            return
        self.is_processing = True
        thread = threading.Thread(target=self.process_videos, daemon=True)
        thread.start()

    # ========== HIGHLIGHT MERGE TAB FUNCTIONS ==========
    
    def apply_all_effects(self, video, filename=""):
        """Apply all effects from settings to a video clip"""
        try:
            # Apply speed
            speed = self.speed_factor.get()
            if speed != 1.0:
                video = video.with_effects([vfx.MultiplySpeed(speed)])
            
            # Apply mirror (horizontal flip)
            if self.mirror_enabled.get():
                video = video.with_effects([vfx.MirrorX()])
            
            # Convert to portrait if enabled
            if self.convert_to_portrait.get() and not self.simple_mode.get():
                video = self.convert_to_portrait_mode(video)
            
            # Apply brightness
            brightness = self.brightness.get()
            if brightness != 1.0:
                def adjust_brightness(image):
                    return np.clip(image * brightness, 0, 255).astype('uint8')
                video = video.image_transform(adjust_brightness)
            
            # Apply zoom
            zoom = self.zoom_factor.get()
            if zoom != 1.0:
                w, h = video.size
                new_w = int(w * zoom)
                new_h = int(h * zoom)
                video = video.resized((new_w, new_h))
                # Crop to original size (center crop)
                x1 = (new_w - w) // 2
                y1 = (new_h - h) // 2
                video = video.cropped(x1=x1, y1=y1, x2=x1+w, y2=y1+h)
            
            # Apply blur
            blur = self.blur_amount.get()
            if blur > 0:
                def apply_blur(image):
                    return gaussian_filter(image, sigma=(blur, blur, 0)).astype('uint8')
                video = video.image_transform(apply_blur)
            
            # Apply black & white
            if self.blackwhite_enabled.get():
                video = video.with_effects([vfx.BlackAndWhite()])
            
            # Apply volume boost
            volume = self.volume_boost.get()
            if volume != 1.0 and video.audio:
                video = video.with_effects([afx.MultiplyVolume(volume)])
            
            # Generate and add subtitles if enabled
            if self.enable_subtitles.get() and not self.simple_mode.get():
                try:
                    # Use Whisper if available, otherwise Google Speech
                    if WHISPER_AVAILABLE and self.ai_engine.get() == "Whisper AI":
                        srt_file = self.generate_subtitles_whisper(video)
                    else:
                        srt_file = self.generate_subtitles(video)
                    
                    if srt_file:
                        # Add subtitles using MoviePy
                        subtitle_clips = self._create_subtitle_clips_from_srt(srt_file)
                        if subtitle_clips:
                            video = CompositeVideoClip([video] + subtitle_clips)
                except Exception as e:
                    print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ th√™m ph·ª• ƒë·ªÅ cho {filename}: {e}")
            
            return video
            
        except Exception as e:
            print(f"‚ùå L·ªói √°p d·ª•ng hi·ªáu ·ª©ng cho {filename}: {e}")
            return video
    
    def _create_subtitle_clips_from_srt(self, srt_file):
        """Create subtitle clips from SRT file"""
        try:
            subtitle_clips = []
            
            with open(srt_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse SRT
            import re
            pattern = r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n(.*?)(?:\n\n|\Z)'
            matches = re.findall(pattern, content, re.DOTALL)
            
            for match in matches:
                index, start_time, end_time, text = match
                
                # Convert time to seconds
                def time_to_seconds(time_str):
                    h, m, s = time_str.split(':')
                    s, ms = s.split(',')
                    return int(h) * 3600 + int(m) * 60 + int(s) + int(ms) / 1000
                
                start = time_to_seconds(start_time)
                end = time_to_seconds(end_time)
                
                # Create text clip
                try:
                    txt_clip = TextClip(
                        text.strip(),
                        font='Arial-Bold',
                        fontsize=32,
                        color='white',
                        stroke_color='black',
                        stroke_width=2,
                        method='caption',
                        size=(700, None)
                    )
                    txt_clip = txt_clip.with_duration(end - start).with_start(start)
                    txt_clip = txt_clip.with_position(('center', 0.85), relative=True)
                    subtitle_clips.append(txt_clip)
                except:
                    # Fallback: use image-based subtitles
                    pass
            
            return subtitle_clips
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói t·∫°o subtitle clips: {e}")
            return []
    

    def browse_highlight_input_dir(self):
        """Browse and select highlights input directory"""
        directory = filedialog.askdirectory(title="Ch·ªçn th∆∞ m·ª•c ch·ª©a video highlights")
        if directory:
            self.highlight_input_dir.set(directory)
            self.load_highlight_files()
    
    def browse_highlight_output_file(self):
        """Browse and select output file for merged video"""
        filename = filedialog.asksaveasfilename(
            title="Ch·ªçn file output",
            defaultextension=".mp4",
            filetypes=[("MP4 files", "*.mp4"), ("All files", "*.*")]
        )
        if filename:
            self.highlight_output_file.set(filename)
    
    def browse_highlight_files(self):
        """Browse and select multiple highlight video files"""
        filenames = filedialog.askopenfilenames(
            title="Ch·ªçn c√°c video highlight ƒë·ªÉ gh√©p",
            filetypes=[
                ("Video files", "*.mp4 *.avi *.mov *.mkv *.flv *.wmv"),
                ("All files", "*.*")
            ]
        )
        if filenames:
            self.copy_highlights_to_input(filenames)
    
    def on_drop_highlights(self, event):
        """Handle drag and drop event for highlights"""
        files = self.root.tk.splitlist(event.data)
        
        # Filter video files
        video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv', '.MP4', '.AVI', '.MOV', '.MKV', '.FLV', '.WMV')
        video_files = [f for f in files if f.lower().endswith(video_extensions)]
        
        if video_files:
            self.copy_highlights_to_input(video_files)
        else:
            self.highlight_drop_status.config(text="‚ùå Kh√¥ng c√≥ video h·ª£p l·ªá!", fg="#ff4444")
            self.root.after(3000, lambda: self.highlight_drop_status.config(
                text="S·∫Ω gh√©p t·∫•t c·∫£ video theo th·ª© t·ª± t√™n file", fg="#666699"))
    
    def copy_highlights_to_input(self, file_paths):
        """Copy selected/dropped highlight videos to input directory"""
        import shutil
        
        # Create highlights directory if it doesn't exist
        input_dir = self.highlight_input_dir.get()
        if not os.path.exists(input_dir):
            os.makedirs(input_dir)
        
        copied_count = 0
        for file_path in file_paths:
            try:
                filename = os.path.basename(file_path)
                dest_path = os.path.join(input_dir, filename)
                
                if not os.path.exists(dest_path):
                    shutil.copy2(file_path, dest_path)
                    copied_count += 1
                    print(f"‚úÖ ƒê√£ copy: {filename}")
                else:
                    print(f"‚ö†Ô∏è File ƒë√£ t·ªìn t·∫°i: {filename}")
            except Exception as e:
                print(f"‚ùå L·ªói copy {filename}: {str(e)}")
        
        # Update status and reload list
        if copied_count > 0:
            self.highlight_drop_status.config(text=f"‚úÖ ƒê√£ th√™m {copied_count} video!", fg="#00ff88")
            self.highlight_drop_label.config(text=f"ƒê√£ th√™m {copied_count} video")
            self.root.after(3000, lambda: self.highlight_drop_label.config(
                text="K√©o th·∫£ c√°c video highlight v√†o ƒë√¢y"))
            self.root.after(3000, lambda: self.highlight_drop_status.config(
                text="S·∫Ω gh√©p t·∫•t c·∫£ video theo th·ª© t·ª± t√™n file", fg="#666699"))
            self.load_highlight_files()
        else:
            self.highlight_drop_status.config(text="‚ö†Ô∏è Kh√¥ng c√≥ video m·ªõi ƒë∆∞·ª£c th√™m", fg="#ffaa00")
            self.root.after(3000, lambda: self.highlight_drop_status.config(
                text="S·∫Ω gh√©p t·∫•t c·∫£ video theo th·ª© t·ª± t√™n file", fg="#666699"))
    
    def load_highlight_files(self):
        """Load and display highlight files in listbox"""
        input_dir = self.highlight_input_dir.get()
        if not os.path.exists(input_dir):
            return
        
        # Get all video files
        video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')
        video_files = [f for f in os.listdir(input_dir) 
                      if f.lower().endswith(video_extensions)]
        
        # Sort alphabetically
        video_files.sort()
        
        # Update listbox
        self.highlight_listbox.delete(0, tk.END)
        if video_files:
            self.highlight_listbox.insert(tk.END, f"üìä T√¨m th·∫•y {len(video_files)} video:")
            self.highlight_listbox.insert(tk.END, "")
            for i, filename in enumerate(video_files, 1):
                self.highlight_listbox.insert(tk.END, f"{i}. {filename}")
        else:
            self.highlight_listbox.insert(tk.END, "üé¨ Ch∆∞a c√≥ video n√†o")
            self.highlight_listbox.insert(tk.END, "")
            self.highlight_listbox.insert(tk.END, "üí° K√©o th·∫£ ho·∫∑c click ƒë·ªÉ ch·ªçn video")
    
    def start_highlight_merge(self):
        """Start merging highlight videos"""
        if self.highlight_processing:
            messagebox.showwarning("C·∫£nh b√°o", "ƒêang gh√©p video, vui l√≤ng ƒë·ª£i!")
            return
        
        self.highlight_processing = True
        thread = threading.Thread(target=self.merge_highlight_videos, daemon=True)
        thread.start()
    
    def merge_highlight_videos(self):
        """Merge all highlight videos with auto-edit effects using FFmpeg (ULTRA FAST)"""
        try:
            input_dir = self.highlight_input_dir.get()
            output_file = self.highlight_output_file.get()
            
            # Get all video files
            video_extensions = ('.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv')
            video_files = [f for f in os.listdir(input_dir) 
                          if f.lower().endswith(video_extensions)]
            
            if not video_files:
                messagebox.showwarning("C·∫£nh b√°o", "Kh√¥ng t√¨m th·∫•y video n√†o trong th∆∞ m·ª•c highlights!")
                self.highlight_processing = False
                return
            
            # Sort alphabetically
            video_files.sort()
            total_videos = len(video_files)
            
            # Update progress
            def update_progress(current, total, message):
                percent = (current / total) * 100 if total > 0 else 0
                self.root.after(0, lambda: self.highlight_progress_bar.config(value=percent))
                self.root.after(0, lambda: self.highlight_progress_label.config(
                    text=f"‚ö° {message} ({current}/{total})"))
            
            update_progress(0, total_videos, "ƒêang chu·∫©n b·ªã")
            
            # Get FFmpeg path
            try:
                from imageio_ffmpeg import get_ffmpeg_exe
                ffmpeg_path = get_ffmpeg_exe()
            except:
                ffmpeg_path = 'ffmpeg'
            
            # Create temp directory for processed videos
            temp_dir = "temp_highlight_merge"
            os.makedirs(temp_dir, exist_ok=True)
            
            # Process each video with FFmpeg
            processed_files = []
            concat_list_file = os.path.join(temp_dir, "concat_list.txt")
            
            for i, filename in enumerate(video_files, 1):
                try:
                    update_progress(i, total_videos, f"ƒêang x·ª≠ l√Ω: {filename}")
                    
                    input_path = os.path.join(input_dir, filename)
                    temp_output = os.path.join(temp_dir, f"processed_{i:03d}.mp4")
                    
                    # Apply all effects using FFmpeg (same as main tab)
                    self.process_highlight_with_ffmpeg(input_path, temp_output)
                    
                    processed_files.append(temp_output)
                    
                except Exception as e:
                    print(f"‚ùå L·ªói x·ª≠ l√Ω {filename}: {str(e)}")
                    # Continue with other videos
            
            if not processed_files:
                messagebox.showerror("L·ªói", "Kh√¥ng th·ªÉ x·ª≠ l√Ω video n√†o!")
                self.highlight_processing = False
                return
            
            # Create concat list file for FFmpeg
            update_progress(total_videos, total_videos, "ƒêang gh√©p video")
            
            with open(concat_list_file, 'w', encoding='utf-8') as f:
                for video_file in processed_files:
                    # Use absolute path and escape for FFmpeg
                    abs_path = os.path.abspath(video_file).replace('\\', '/')
                    f.write(f"file '{abs_path}'\n")
            
            # Create output directory if needed
            output_dir = os.path.dirname(output_file)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            # Concatenate videos using FFmpeg concat demuxer (FASTEST METHOD)
            update_progress(total_videos, total_videos, "ƒêang xu·∫•t video cu·ªëi c√πng")
            
            # Get codec settings
            codec = 'h264_nvenc' if self.use_gpu.get() else 'libx264'
            
            concat_cmd = [
                ffmpeg_path, '-y',
                '-f', 'concat',
                '-safe', '0',
                '-i', concat_list_file,
                '-c:v', codec,
                '-preset', 'fast',
                '-b:v', '3000k',
                '-c:a', 'aac',
                '-b:a', '192k',
                '-movflags', '+faststart',
                output_file
            ]
            
            # Run FFmpeg concat
            result = subprocess.run(
                concat_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
            )
            
            if result.returncode != 0:
                error_msg = result.stderr.decode('utf-8', errors='ignore')
                raise Exception(f"FFmpeg concat failed: {error_msg[-500:]}")
            
            # Cleanup temp files
            import shutil
            try:
                shutil.rmtree(temp_dir)
            except:
                pass
            
            # Success
            update_progress(total_videos, total_videos, "Ho√†n th√†nh!")
            messagebox.showinfo("Th√†nh c√¥ng", 
                              f"ƒê√£ gh√©p {total_videos} video th√†nh c√¥ng!\n\nFile: {output_file}")
            
        except Exception as e:
            messagebox.showerror("L·ªói", f"ƒê√£ x·∫£y ra l·ªói khi gh√©p video: {str(e)}")
            print(f"‚ùå Chi ti·∫øt l·ªói: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.highlight_processing = False
            self.root.after(0, lambda: self.highlight_progress_bar.config(value=0))
            self.root.after(0, lambda: self.highlight_progress_label.config(
                text="‚ö° S·∫µn s√†ng gh√©p video..."))
    
    def process_highlight_with_ffmpeg(self, input_path, output_path):
        """Process a single highlight video with FFmpeg (apply all effects)"""
        try:
            from imageio_ffmpeg import get_ffmpeg_exe
            ffmpeg_path = get_ffmpeg_exe()
        except:
            ffmpeg_path = 'ffmpeg'
        
        # Build FFmpeg filter chain (same as main tab)
        filters = []
        
        # 1. Speed
        speed_val = self.speed_factor.get()
        if speed_val != 1.0:
            filters.append(f"setpts={1/speed_val}*PTS")
        
        # 2. Mirror (horizontal flip)
        if self.mirror_enabled.get():
            filters.append("hflip")
        
        # 3. Black & White
        if self.blackwhite_enabled.get():
            filters.append("hue=s=0")
        
        # 4. Saturation
        sat_val = self.saturation.get()
        if sat_val != 1.0:
            filters.append(f"eq=saturation={sat_val}")
        
        # 5. Brightness
        bright_val = self.brightness.get()
        if bright_val != 1.0:
            filters.append(f"eq=brightness={bright_val-1.0}")
        
        # 6. Contrast
        contrast_val = self.contrast.get()
        if contrast_val != 1.0:
            filters.append(f"eq=contrast={contrast_val}")
        
        # 7. Blur
        blur_val = self.blur_amount.get()
        if blur_val > 0:
            filters.append(f"boxblur={blur_val}:{blur_val}")
        
        # 8. Zoom (crop)
        zoom_val = self.zoom_factor.get()
        if zoom_val != 1.0:
            filters.append(f"scale=iw*{zoom_val}:ih*{zoom_val}")
            filters.append(f"crop=iw/{zoom_val}:ih/{zoom_val}")
        
        # 9. Portrait mode with blur background
        if self.convert_to_portrait.get() and not self.simple_mode.get():
            # Complex filter for portrait mode
            filters.append("scale=720:1280:force_original_aspect_ratio=decrease")
            filters.append("pad=720:1280:(ow-iw)/2:(oh-ih)/2:black")
        else:
            # Simple scale
            filters.append("scale=720:-1:force_original_aspect_ratio=decrease")
        
        # Combine filters
        vf = ','.join(filters) if filters else None
        
        # Audio filters
        audio_filters = []
        
        # Volume boost
        vol_val = self.volume_boost.get()
        if vol_val != 1.0:
            audio_filters.append(f"volume={vol_val}")
        
        # Speed (audio)
        if speed_val != 1.0:
            audio_filters.append(f"atempo={speed_val}")
        
        # Bass Boost
        bass_boost = self.bass_boost.get()
        if bass_boost > 0:
            audio_filters.append(f"bass=g={bass_boost}")
        
        # Treble Boost
        treble_boost = self.treble_boost.get()
        if treble_boost > 0:
            audio_filters.append(f"treble=g={treble_boost}")
        
        af = ','.join(audio_filters) if audio_filters else None
        
        # Build FFmpeg command
        cmd = [
            ffmpeg_path, '-y',
            '-i', input_path
        ]
        
        if vf:
            cmd.extend(['-vf', vf])
        
        if af:
            cmd.extend(['-af', af])
        
        # Get codec settings
        codec = 'h264_nvenc' if self.use_gpu.get() else 'libx264'
        
        cmd.extend([
            '-c:v', codec,
            '-preset', 'fast',
            '-b:v', '3000k',
            '-r', '15',
            '-c:a', 'aac',
            '-b:a', '192k',
            output_path
        ])
        
        # Run FFmpeg
        result = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            creationflags=subprocess.CREATE_NO_WINDOW if sys.platform == 'win32' else 0
        )
        
        if result.returncode != 0:
            error_msg = result.stderr.decode('utf-8', errors='ignore')
            raise Exception(f"FFmpeg processing failed: {error_msg[-500:]}")





def main():
    # Use TkinterDnD if available for drag & drop support
    if TKDND_AVAILABLE:
        root = TkinterDnD.Tk()
    else:
        root = tk.Tk()
    
    app = VideoEditorGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()